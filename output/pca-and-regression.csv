ID,Source,Type,Category,Stage
0,"**The idea is:**

 - Feature reduction with PCA
 - Data transformation (log, hot encoding, nan)
 - Test different regression models

**Things found:**

- Applying log transformation really increases the accuracy.
- Using PCA with 36 components makes the learning and testing much (much much) faster.
- Removing columns with more than 1000 NaNs gives better result than applying ""mean"" to them.
- There are outliers. Instead of removing them, using Huber seems to provide a good result. Huber is a model robust to outliers.",markdown,,
1,"import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.decomposition import PCA
from sklearn.preprocessing import Imputer
from sklearn.model_selection import KFold
from sklearn import linear_model
from sklearn.metrics import make_scorer
from sklearn.ensemble import BaggingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn import svm
from sklearn.metrics import r2_score
from sklearn.ensemble import AdaBoostRegressor
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import tflearn
import tensorflow as tf
import seaborn
import warnings
warnings.filterwarnings('ignore')

from subprocess import check_output
print(check_output([""ls"", ""../input""]).decode(""utf8""))",code,,
2,"## Data Load ##

I mix data and test to manipulate all the data just once. SalePrice is extracted to its own variable ""labels"". Finally, SalesPrice is remove from data.",markdown,,
3,"train = pd.read_csv('../input/train.csv')
labels=train[""SalePrice""]
test = pd.read_csv('../input/test.csv')
data = pd.concat([train,test],ignore_index=True)
data = data.drop(""SalePrice"", 1)
ids = test[""Id""]",code,,
4,train.head(),code,,
5,"# Count the number of rows in train
train.shape[0]",code,,
6,"# Count the number of rows in total
data.shape[0]",code,,
7,"# Count the number of NaNs each column has.
nans=pd.isnull(data).sum()
nans[nans>0]",code,,
8,"# Remove id and columns with more than a thousand missing values
data=data.drop(""Id"", 1)
data=data.drop(""Alley"", 1)
data=data.drop(""Fence"", 1)
data=data.drop(""MiscFeature"", 1)
data=data.drop(""PoolQC"", 1)
data=data.drop(""FireplaceQu"", 1)",code,,
9,"# Count the column types
data.dtypes.value_counts()",code,,
10,"## Data Manipulation ##

- Apply hot encoding, convert categorical variable into dummy/indicator variables.
- Fill NaN with median for that column.
- Log transformation.
- Change -inf to 0.",markdown,,
11,"all_columns = data.columns.values
non_categorical = [""LotFrontage"", ""LotArea"", ""MasVnrArea"", ""BsmtFinSF1"", 
                   ""BsmtFinSF2"", ""BsmtUnfSF"", ""TotalBsmtSF"", ""1stFlrSF"", 
                   ""2ndFlrSF"", ""LowQualFinSF"", ""GrLivArea"", ""GarageArea"", 
                   ""WoodDeckSF"", ""OpenPorchSF"", ""EnclosedPorch"", ""3SsnPorch"", 
                   ""ScreenPorch"",""PoolArea"", ""MiscVal""]

categorical = [value for value in all_columns if value not in non_categorical]",code,,
12,"#Â One Hot Encoding and nan transformation
data = pd.get_dummies(data)

imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)
data = imp.fit_transform(data)

# Log transformation
data = np.log(data)
labels = np.log(labels)

# Change -inf to 0 again
data[data==-np.inf]=0",code,,
13,"## Feature reduction ##

There are many features, so I am going to use PCA to reduce them. The idea is to start with n_components = number of columns. Then select the number of components that add up to 1 variance_ratio.",markdown,,
14,"pca = PCA(whiten=True)
pca.fit(data)
variance = pd.DataFrame(pca.explained_variance_ratio_)
np.cumsum(pca.explained_variance_ratio_)",code,,
15,"pca = PCA(n_components=36,whiten=True)
pca = pca.fit(data)
dataPCA = pca.transform(data)",code,,
16,"## Data Model Selection ##

Simple test to run multiple models against our data. First, with raw features. No PCA.",markdown,,
17,"# Split traing and test
train = data[:1460]
test = data[1460:]",code,,
18,"# R2 Score

def lets_try(train,labels):
    results={}
    def test_model(clf):
        
        cv = KFold(n_splits=5,shuffle=True,random_state=45)
        r2 = make_scorer(r2_score)
        r2_val_score = cross_val_score(clf, train, labels, cv=cv,scoring=r2)
        scores=[r2_val_score.mean()]
        return scores

    clf = linear_model.LinearRegression()
    results[""Linear""]=test_model(clf)
    
    clf = linear_model.Ridge()
    results[""Ridge""]=test_model(clf)
    
    clf = linear_model.BayesianRidge()
    results[""Bayesian Ridge""]=test_model(clf)
    
    clf = linear_model.HuberRegressor()
    results[""Hubber""]=test_model(clf)
    
    clf = linear_model.Lasso(alpha=1e-4)
    results[""Lasso""]=test_model(clf)
    
    clf = BaggingRegressor()
    results[""Bagging""]=test_model(clf)
    
    clf = RandomForestRegressor()
    results[""RandomForest""]=test_model(clf)
    
    clf = AdaBoostRegressor()
    results[""AdaBoost""]=test_model(clf)
    
    clf = svm.SVR()
    results[""SVM RBF""]=test_model(clf)
    
    clf = svm.SVR(kernel=""linear"")
    results[""SVM Linear""]=test_model(clf)
    
    results = pd.DataFrame.from_dict(results,orient='index')
    results.columns=[""R Square Score""] 
    results=results.sort(columns=[""R Square Score""],ascending=False)
    results.plot(kind=""bar"",title=""Model Scores"")
    axes = plt.gca()
    axes.set_ylim([0.5,1])
    return results

lets_try(train,labels)",code,,
19,"Now, let's try the same but using data with PCA applied.",markdown,,
20,"# Split traing and test
train = dataPCA[:1460]
test = dataPCA[1460:]

lets_try(train,labels)",code,,
21,"cv = KFold(n_splits=5,shuffle=True,random_state=45)

parameters = {'alpha': [1000,100,10],
              'epsilon' : [1.2,1.25,1.50],
              'tol' : [1e-10]}

clf = linear_model.HuberRegressor()
r2 = make_scorer(r2_score)
grid_obj = GridSearchCV(clf, parameters, cv=cv,scoring=r2)
grid_fit = grid_obj.fit(train, labels)
best_clf = grid_fit.best_estimator_ 

best_clf.fit(train,labels)",code,,
22,"Simple Neural Network
---------------------

Now I am going to try a simple neural network, to see if i can improve the result.",markdown,,
23,"# Shape the labels
labels_nl = labels
labels_nl = labels_nl.reshape(-1,1)",code,,
24,"tf.reset_default_graph()
r2 = tflearn.R2()
net = tflearn.input_data(shape=[None, train.shape[1]])
net = tflearn.fully_connected(net, 30, activation='linear')
net = tflearn.fully_connected(net, 10, activation='linear')
net = tflearn.fully_connected(net, 1, activation='linear')
sgd = tflearn.SGD(learning_rate=0.1, lr_decay=0.01, decay_step=100)
net = tflearn.regression(net, optimizer=sgd,loss='mean_square',metric=r2)
model = tflearn.DNN(net)",code,,
25,"model.fit(train, labels_nl,show_metric=True,validation_set=0.2,shuffle=True,n_epoch=50)",code,,
26,"# Make predictions

predictions_huber = best_clf.predict(test)
predictions_DNN = model.predict(test)
predictions_huber = np.exp(predictions_huber)
predictions_DNN = np.exp(predictions_DNN)
predictions_DNN = predictions_DNN.reshape(-1,)

sub = pd.DataFrame({
        ""Id"": ids,
        ""SalePrice"": predictions_DNN
    })

sub.to_csv(""prices_submission.csv"", index=False)
#print(sub)",code,,
27,,code,,
