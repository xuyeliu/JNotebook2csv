{"cells":[{"metadata":{"_uuid":"c832a94ddf3e502f1bbd1568c8d0288ca05d60b7"},"cell_type":"markdown","source":"# House Prices: Advanced Regression Techniques\n## *Top 7% using ElasticNet with Interactions and Feature Engineering*"},{"metadata":{"_uuid":"f84b7625f9f4513b5f4823565c2159965cf67d98"},"cell_type":"markdown","source":"This notebook represents my first proper attempt at a regression problem in python/sklearn, on the Ames housing dataset. It achieves a leaderboard score of 0.11553 with a linear regression model (ElasticNet and Lasso give similar results). This places it within the top 7% of the leaderboard, without using an ensemble model or anything more complicated than linear regression. I was quite quickly able to get to 0.12, but the extra 0.004 took a lot of model optimisation, and the addition of a few new features including some interaction terms.\n\nRoughly speaking, the steps included in the notebook are:\n\n**Data processing:**\n* [Dealing with missing values](#nanvalues)\n* [Dealing with zeros](#zeros)\n* Encoding categorical variables: [1](#bsmtfinish), [2](#categorder) and [3](#dummies)\n* Creating new features: [1](#bsmtfinish) and [2](#1st2ndflr)\n* Transforming features: [1](#transformSP) and [2](#transformFeats)\n\n**Exploratory Data Analysis:**\n* [Distribution of SalePrice for all the features](#distribFeats)\n* [Correlation between the features and SalePrice (or each other)](#correl)\n* Significance of the SalePrice variation within each feature: [1](#sigcateg) and [2](#sigdiscrete)\n\n**Model Fitting:**\n* [Outlier removal](#outliers)\n* [Optimisation of different models](#modelfit)\n* [Feature interactions](#interactions)\n\nThere are a lot of great kernels already which share similar features with a lot of what I've done, such as feature transformations etc. However, there are some things I believe I've approached a bit differently to the other kernels I've seen. So for people who have already read through other kernels, the parts of the notebook that may be the most interesting ones to look at are:\n* Filling LotFrontage NaN values using a linear regression model. See [here](#lotfrontagenan).\n* Creation of features for different basement finish types. See [here](#bsmtfinish).\n* Adding dummy variables for features with a lot of zeros to improve model fits. See [here](#zeros).\n* Outlier removal based on the residuals between predicted and true sale prices in the training data. See [here](#outliers).\n* Inclusion of interaction terms in the model, using Lasso to decide which interactions are important. See [here](#interactions).\n\n\nSo, let's get started!"},{"metadata":{"_uuid":"b6884a67ed3d323781ec6ee1e93e4099ebfa0704"},"cell_type":"markdown","source":"## Module Imports"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"62a0f94be1bc1a105a2dd74715d69eb3bdd28ea8"},"cell_type":"code","source":"# pandas for data loading, manipulation etc.\nimport pandas as pd\n\n# numeric functions\nimport numpy as np\nfrom scipy import stats\nfrom math import ceil\n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# modelling\nfrom sklearn.model_selection import GridSearchCV, RepeatedKFold, cross_val_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.svm import LinearSVR, SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import PolynomialFeatures","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"bdece232a11b886e5b4301fe2e9dfcbd02bf0a60"},"cell_type":"markdown","source":"## Load Data\n\nLoad the train and test data in to pandas data frames. Combine them in to a single data frame so it is easy to apply any feature transformations we use to both sets at the same time."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"890f9e4ccfe37694816619574d3bb3154c53df0a","collapsed":true},"cell_type":"code","source":"# Load the data\ndf_train = pd.read_csv('../input/train.csv',index_col='Id')\ndf_test = pd.read_csv('../input/test.csv',index_col='Id')\n\n# ids of full training dataset\nid_train = df_train.index    \n\n# ids of full test dataset\nid_test = df_test.index\n\n# combine train and test datas in to one dataframe\ndf_all = pd.concat([df_train,df_test], sort=True)\ndf_all.head(5)","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"eb4a30bda9777c5fbcb547bc8538bbdcd29a0854"},"cell_type":"markdown","source":"<a id='nanvalues'></a>\n## Columns with NaN Values\n\nFirst thing to notice is that the data contains a lot of NaN values, which need to be dealt with."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"258c8d51883f59eb744736e01931bdf46fe7a7bc","collapsed":true},"cell_type":"code","source":"# summary of columns with nan values\n#SalePrice nans: test data\n\ncols_with_na = df_all.isnull().sum()\ncols_with_na = cols_with_na[cols_with_na>0]\nprint(cols_with_na.sort_values(ascending=False))","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"291d41c62790b3be559f7fdc20dbeeb2baf4319d"},"cell_type":"markdown","source":"## Meaningful NaN Values\n\nLooking at the data description file, it can be seen that a lot of the NaNs are not true missing values, but rather indicate that the property does not have that feature. E.g. a property with NaN garage type has no garage.\n\nWe want to preserve this information, so I replace the NaNs with the string \"None\" in the categorical features, or with zero for the numeric features, where this applies."},{"metadata":{"trusted":true,"_uuid":"3a53b4784851069e8206f1d56a1e43f332c227c4","collapsed":true},"cell_type":"code","source":"# columns where NaN values have meaning e.g. no pool etc.\ncols_fillna = ['PoolQC','MiscFeature','Alley','Fence','MasVnrType','FireplaceQu',\n               'GarageQual','GarageCond','GarageFinish','GarageType',\n               'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2']\n\n# replace 'NaN' with 'None' in these columns\nfor col in cols_fillna:\n    df_all[col].fillna('None',inplace=True)\n    \n#GarageYrBlt nans: no garage. Fill with property YearBuilt.\n#(more appropriate than 0, which would be ~2000 away from all other values)\ndf_all.loc[df_all.GarageYrBlt.isnull(),'GarageYrBlt'] = df_all.loc[df_all.GarageYrBlt.isnull(),'YearBuilt']\n\n#No masonry veneer - fill area with 0\ndf_all.MasVnrArea.fillna(0,inplace=True)\n\n#No basement - fill areas/counts with 0    \ndf_all.BsmtFullBath.fillna(0,inplace=True)\ndf_all.BsmtHalfBath.fillna(0,inplace=True)\ndf_all.BsmtFinSF1.fillna(0,inplace=True)\ndf_all.BsmtFinSF2.fillna(0,inplace=True)\ndf_all.BsmtUnfSF.fillna(0,inplace=True)\ndf_all.TotalBsmtSF.fillna(0,inplace=True)\n\n#No garage - fill areas/counts with 0\ndf_all.GarageArea.fillna(0,inplace=True)\ndf_all.GarageCars.fillna(0,inplace=True)\n\ndf_all[cols_fillna].head(5)","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"a30e754136370230d94badf1f68b40a3d85b5611"},"cell_type":"markdown","source":"<a id='lotfrontagenan'></a>\n## LotFrontage NaN Values\n\nThe most difficult NaNs to deal with are those in the LotFrontage column. The simplest thing to do would be to replace the NaNs with the mean or median LotFrontage. For a bit of extra practice with sklearn, and to hopefully give a more accurate estimate, I decided to fit a Ridge model (linear regression with regularisation) to predict the missing values. I don't include the SalePrice in the model, to avoid leakage between the train and test datasets.\n\nThe model isn't perfect, but hopefully better than just using the mean. Features relating to the size and shape of the lot appear amongst the largest coefficients, which makes sense, and the predicted values look sensible."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"912e9d8ac1fc1b90899625ae5077c2f1f2642b86"},"cell_type":"code","source":"# function to normalise a column of values to lie between 0 and 1\ndef scale_minmax(col):\n    return (col-col.min())/(col.max()-col.min())","execution_count":7,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"f9c5d45f29b5c178c83b3a0fa0d425846a1e4e08","collapsed":true},"cell_type":"code","source":"#LotFrontage\n# fill nan values using a linear regressor\n\n# convert categoricals to dummies, exclude SalePrice from model\ndf_frontage = pd.get_dummies(df_all.drop('SalePrice',axis=1))\n\n# normalise columns to 0-1\nfor col in df_frontage.drop('LotFrontage',axis=1).columns:\n    df_frontage[col] = scale_minmax(df_frontage[col])\n\nlf_train = df_frontage.dropna()\nlf_train_y = lf_train.LotFrontage\nlf_train_X = lf_train.drop('LotFrontage',axis=1)  \n\n# fit model\nlr = Ridge()\nlr.fit(lf_train_X, lf_train_y)\n\n# check model results\nlr_coefs = pd.Series(lr.coef_,index=lf_train_X.columns)\n\nprint('----------------')\nprint('Intercept:',lr.intercept_)\nprint('----------------')\nprint(lr_coefs.sort_values(ascending=False).head(10))\nprint('----------------')\nprint(lr_coefs.sort_values(ascending=False).tail(10))\nprint('----------------')\nprint('R2:',lr.score(lf_train_X,lf_train_y))\nprint('----------------')\n\nlf_pred_y = lr.predict(lf_train_X)\nplt.figure(figsize=(18,5))\nplt.subplot(1,3,1)\nplt.plot(lf_train_y,lf_pred_y,'.')\nplt.xlabel('y')\nplt.ylabel('y_pred');\nplt.title('Prediction vs. True Value')\n\nplt.subplot(1,3,2)\nplt.plot(lf_train_y, lf_train_y-lf_pred_y,'.')\nplt.xlabel('y')\nplt.ylabel('y - y_pred');\nplt.title('Residual vs. True Value')\n\n\n# fill na values using model predictions\nnan_frontage = df_all.LotFrontage.isnull()\nX = df_frontage[nan_frontage].drop('LotFrontage',axis=1)\ny = lr.predict(X)\n\nplt.subplot(1,3,3)\nplt.hist(y,bins=20)\nplt.title('Predicted Lot Frontages for NaN Values')\n\n# fill nan values\ndf_all.loc[nan_frontage,'LotFrontage'] = y","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"7c80f77b0e89e12da7f1a53a3d9c72bd93c0d736"},"cell_type":"markdown","source":"## Remaining NaNs\n\nNow we're just left with a handful of remaining features/properties that have NaN values. All of them happen to be categorical variables, and I just replace the missing values with the mode for that feature. Most of the remaining NaNs are in the test dataset, otherwise I might have just deleted them."},{"metadata":{"trusted":true,"_uuid":"f44452b140129ed4d3bcc29f30351e660933fc96","collapsed":true},"cell_type":"code","source":"# Remaining Nan values\ncols_with_na = df_all.drop('SalePrice',axis=1).isnull().sum()\ncols_with_na = cols_with_na[cols_with_na>0]\n\nprint(cols_with_na.sort_values(ascending=False))\n\nrows_with_na = df_all.drop('SalePrice',axis=1).isnull().sum(axis=1)\nrows_with_na = rows_with_na[rows_with_na>0]\nprint(rows_with_na.sort_values(ascending=False))\n\n# fill remaining nans with mode in that column\nfor col in cols_with_na.index:\n    df_all[col].fillna(df_all[col].mode()[0], inplace=True)","execution_count":9,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"93c7a16218004d8867a787468dde436e35777ae3","collapsed":true},"cell_type":"code","source":"# Now no more NaN values apart from SalePrice in test data\ndf_all.info()","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"b71292c752b587c7e4a016161647e28644d9d3fb"},"cell_type":"markdown","source":"<a id='bsmtfinish'></a>\n## Basement Finish Types\n\nWith all the NaN values dealt with, it's time to start investigating and playing with the data to get it in to the format we want for model fitting. Note that the final order things appear in the notebook is not the order I did them at first - a lot of time was spent exploring the data (looking at e.g. the scatter and distribution plots shown later) before I decided on how to fill the NaNs (above), and the feature manipulations below.\n\nIn the basement features there are two areas representing a certain finish type, e.g. an area that is converted to \"Good living quarters\", and an area that is unfinished. The finish type and areas are in separate columns, and a simple model wouldn't catch the interaction between the type and area. So I create new columns representing the area of each finish type the property's basement has, and remove the original columns.\n\nI also realised the area by itself may not be the most useful stat - large properties will have large areas and high sale prices 'by default'. What might be more interesting is the *fraction* of the basement that is of each type. I create more new columns for these fractions.\n\nIn the plots below there is a slight positive correlation between the unfinished area and sale price (as larger properties are more likely to have large unfinished areas), but a slight negative correlation between the unfinished *fraction* and the sale price, as having the basement unfinished is undesirable."},{"metadata":{"trusted":true,"_uuid":"ed39a488bffeffe900e7147ee6e7791a2c2c6ca1","collapsed":true},"cell_type":"code","source":"# create separate columns for area of each possible\n# basement finish type\nbsmt_fin_cols = ['BsmtGLQ','BsmtALQ','BsmtBLQ',\n                 'BsmtRec','BsmtLwQ']\n\nfor col in bsmt_fin_cols:\n    # initialise as columns of zeros\n    df_all[col+'SF'] = 0\n\n# fill remaining finish type columns\nfor row in df_all.index:\n    fin1 = df_all.loc[row,'BsmtFinType1']\n    if (fin1!='None') and (fin1!='Unf'):\n        # add area (SF) to appropriate column\n        df_all.loc[row,'Bsmt'+fin1+'SF'] += df_all.loc[row,'BsmtFinSF1']\n        \n    fin2 = df_all.loc[row,'BsmtFinType2']\n    if (fin2!='None') and (fin2!='Unf'):\n        df_all.loc[row,'Bsmt'+fin2+'SF'] += df_all.loc[row,'BsmtFinSF2']\n\n\n# remove initial BsmtFin columns\ndf_all.drop(['BsmtFinType1','BsmtFinSF1','BsmtFinType2','BsmtFinSF2'], axis=1, inplace=True)\n\n# already have BsmtUnf column in dataset\nbsmt_fin_cols.append('BsmtUnf')\n\n# also create features representing the fraction of the basement that is each finish type\nfor col in bsmt_fin_cols:\n    df_all[col+'Frac'] = df_all[col+'SF']/df_all['TotalBsmtSF']\n    # replace any nans with zero (for properties without a basement)\n    df_all[col+'Frac'].fillna(0,inplace=True)\n\n#plot to demonstrate the effect of the new feature\nplt.figure(figsize=(15,4))\nplt.subplot(1,2,1)\nsns.regplot(df_all.loc[(df_all['BsmtUnfFrac']>0) & (df_all['BsmtUnfFrac']<1), 'BsmtUnfSF'],\n            df_all.loc[(df_all['BsmtUnfFrac']>0) & (df_all['BsmtUnfFrac']<1), 'SalePrice'],\n            scatter_kws={'marker':'.','s':3,'alpha':0.5,'color':'r'},\n            line_kws={'color':'k'})\n\nplt.xlabel('BsmtUnf SF')\nplt.title('Before')\n\nplt.subplot(1,2,2)\nsns.regplot(df_all.loc[(df_all['BsmtUnfFrac']>0) & (df_all['BsmtUnfFrac']<1), 'BsmtUnfFrac'],\n            df_all.loc[(df_all['BsmtUnfFrac']>0) & (df_all['BsmtUnfFrac']<1), 'SalePrice'],            \n            scatter_kws={'marker':'.','s':3,'alpha':0.5,'color':'b'},\n            line_kws={'color':'k'})\n\nplt.xlabel('BsmtUnf Fraction')\nplt.title('After')\n\n\ndf_all.loc[:,df_all.columns.str.contains('Bsmt')].head(5)","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"bb6d21f55400eadfa2bd4a6f37b13fcec647bb0b"},"cell_type":"markdown","source":"<a id='1st2ndflr'></a>\n## 1st and 2nd Floor Area\n\nLike the basement, I thought the fraction of each property's area that is on the 1st floor, 2nd floor, or unfinished may be useful, so I create columns for these.\n\nI also create columns to indicate the total size of the property - TotalAreaSF for the total size of all interior areas, LivingAreaSF for the total area of living quarters (including any in the basement), and StorageAreaSF for total area of non-living quarter interiors (garage etc.)"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"09b1ebe542c04d10e57325535a6e7ba83f2d79d8"},"cell_type":"code","source":"df_all['LowQualFinFrac'] = df_all['LowQualFinSF']/df_all['GrLivArea']\ndf_all['1stFlrFrac'] = df_all['1stFlrSF']/df_all['GrLivArea']\ndf_all['2ndFlrFrac'] = df_all['2ndFlrSF']/df_all['GrLivArea']\n\ndf_all['TotalAreaSF'] = df_all['GrLivArea']+df_all['TotalBsmtSF']+df_all['GarageArea']+df_all['EnclosedPorch']+df_all['ScreenPorch']\ndf_all['LivingAreaSF'] = df_all['1stFlrSF'] + df_all['2ndFlrSF'] + df_all['BsmtGLQSF'] + df_all['BsmtALQSF'] + df_all['BsmtBLQSF']\ndf_all['StorageAreaSF'] = df_all['LowQualFinSF'] + df_all['BsmtRecSF'] + df_all['BsmtLwQSF'] + df_all['BsmtUnfSF'] + df_all['GarageArea']","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"2a6322131a39e2d3af4720d69327ce9fe54f8cf1"},"cell_type":"markdown","source":"<a id='categorder'></a>\n## Categorical Features with Meaningful Ordering\n\nSome of the categorical variables can directly be converted in to numeric scales. For example, there are many 'quality' features, that have values ranging from \"Excellent\" to \"Poor\". These can be converted in to integer scales, with the most desirable descriptions given the highest scores.\n\nIn some cases I make an educated guess at what is most desirable, e.g. I assume it is better to have a property that is not on a slope."},{"metadata":{"trusted":true,"_uuid":"be62260f8742c0879da50451e9c0f99c011017f5","collapsed":true},"cell_type":"code","source":"# convert some categorical values to numeric scales\n\n#Excellent, Good, Typical, Fair, Poor, None: Convert to 0-5 scale\ncols_ExGd = ['ExterQual','ExterCond','BsmtQual','BsmtCond',\n             'HeatingQC','KitchenQual','FireplaceQu','GarageQual',\n            'GarageCond','PoolQC']\n\ndict_ExGd = {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0}\n\nfor col in cols_ExGd:\n    df_all[col].replace(dict_ExGd, inplace=True)\n\ndisplay(df_all[cols_ExGd].head(5))    \n\n# Remaining columns\ndf_all['BsmtExposure'].replace({'Gd':4,'Av':3,'Mn':2,'No':1,'None':0}, inplace=True)\n\ndf_all['CentralAir'].replace({'Y':1,'N':0}, inplace=True)\n\ndf_all['Functional'].replace({'Typ':7,'Min1':6,'Min2':5,'Mod':4,'Maj1':3,'Maj2':2,'Sev':1,'Sal':0}, inplace=True)\n\ndf_all['GarageFinish'].replace({'Fin':3,'RFn':2,'Unf':1,'None':0}, inplace=True)\n\ndf_all['LotShape'].replace({'Reg':3,'IR1':2,'IR2':1,'IR3':0}, inplace=True)\n\ndf_all['Utilities'].replace({'AllPub':3,'NoSewr':2,'NoSeWa':1,'ELO':0}, inplace=True)\n\ndf_all['LandSlope'].replace({'Gtl':2,'Mod':1,'Sev':0}, inplace=True)","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"3e4326ccf21746292dadd48baf971165a1b2e3f0"},"cell_type":"markdown","source":"<a id='zeros'></a>\n## Dealing with Zeros\n\nAll those NaNs I replaced with zeros earlier can actually be a bit of a nuisance. In a linear regression model, the large number of properties with, for example, no 2nd floor, and so zero 2nd floor area, will bias the estimated dependence between sale price and 2nd floor area.\n\nTo give the models flexibility to get around this issue, for features with a lot of zeros I add a dummy variable to indicate whether that feature is present in the property or not. The effect of doing this is shown by the plot below. For a couple of features where very few properties have it, I remove the original feature and only keep the dummy variable. I also decided to count the \"half bathroom\" features literally as 0.5 bathrooms, added them to the \"full bathroom\" totals, then dropped the \"half bathroom\" columns."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"d0f74937ddc11ef26679e3f5b740342e0551aa08","collapsed":true},"cell_type":"code","source":"# fraction of zeros in each column\nfrac_zeros = ((df_all==0).sum()/len(df_all))\n\n# no. unique values in each column\nn_unique = df_all.nunique()\n\n# difference between frac. zeros and expected\n# frac. zeros if values evenly distributed between\n# classes\nxs_zeros = frac_zeros - 1/n_unique\n\n# create dataframe and display which columns may be problematic\nzero_cols = pd.DataFrame({'frac_zeros':frac_zeros,'n_unique':n_unique,'xs_zeros':xs_zeros})\nzero_cols = zero_cols[zero_cols.frac_zeros>0]\nzero_cols.sort_values(by='xs_zeros',ascending=False,inplace=True)\ndisplay(zero_cols[(zero_cols.xs_zeros>0)])\n\n#very few properties with Pool or 3SsnPorch\n#replace columns with binary indicator\ndf_all['HasPool'] = (df_all['PoolQC']>0).astype(int)\ndf_all['Has3SsnPorch'] = (df_all['3SsnPorch']>0).astype(int)\ndf_all.drop(['PoolQC','PoolArea','3SsnPorch'],axis=1,inplace=True)\n\n# 'half' bathrooms - add half value to 'full' bathrooms\ndf_all['BsmtFullBath'] = df_all['BsmtFullBath'] + 0.5*df_all['BsmtHalfBath']\ndf_all['FullBath'] = df_all['FullBath'] + 0.5*df_all['HalfBath']\ndf_all.drop(['BsmtHalfBath','HalfBath'],axis=1,inplace=True)\n\n# create additional dummy variable for\n# continuous variables with a lot of zeros\ndummy_cols = ['LowQualFinSF','2ndFlrSF',\n              'MiscVal','ScreenPorch','WoodDeckSF','OpenPorchSF',\n              'EnclosedPorch','MasVnrArea','GarageArea','Fireplaces',             \n              'BsmtGLQSF','BsmtALQSF','BsmtBLQSF','BsmtRecSF',\n              'BsmtLwQSF','BsmtUnfSF','TotalBsmtSF']\n\nfor col in dummy_cols:\n    df_all['Has'+col] = (df_all[col]>0).astype(int)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c68b89def6a281292604f4061cd2ea15ade88c0a","collapsed":true},"cell_type":"code","source":"#PROBLEM OF LOTS OF ZEROS EXAMPLE -2ndFlrSF\n#####\n# mode only using 2ndflrsf\n#####\ntmp = scale_minmax(df_all[['SalePrice','2ndFlrSF','Has2ndFlrSF']].dropna())\n\ny = tmp.SalePrice\nX = tmp['2ndFlrSF']\n\nplt.plot(X,y,'.',label='data',alpha=0.5)\n\nlr = LinearRegression()\nlr.fit(X.values.reshape(-1, 1),y)\nlr_coefs = pd.Series(lr.coef_,index=['Has2ndFlrSF'])\nlr_intercept = lr.intercept_\n\ndef regval(flr2ndSF):\n    return flr2ndSF*lr_coefs + lr_intercept\n\nplt.plot([0,1],[regval(0),regval(1)],'b',linewidth=3,label='2ndFlr only')\n\n#####\n# model using has2ndflr dummy variable\n#####\ntmp = scale_minmax(df_all[['SalePrice','2ndFlrSF','Has2ndFlrSF']].dropna())\n\ny = tmp.SalePrice\n\nX = tmp.drop('SalePrice',axis=1)\n\nlr = LinearRegression()\nlr.fit(X,y)\nlr_coefs = pd.Series(lr.coef_,index=X.columns)\nlr_intercept = lr.intercept_\n\ndef regval(flr2ndSF,has2nd):\n    return flr2ndSF*lr_coefs['2ndFlrSF'] + has2nd*lr_coefs['Has2ndFlrSF'] + lr_intercept\n\nplt.plot([0,0.02,1],[regval(0,0),regval(0.02,1),regval(1,1)],'r',linewidth=3,label='with Has2ndFlr')\n\nplt.legend()\nplt.xlabel('2ndFlrSF')\nplt.ylabel('SalePrice');","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"6f5b8a3114aceff6c44ac89d6b0c8b3e8c7dda6f"},"cell_type":"markdown","source":"<a id='transformSP'></a>\n## Distribution of SalePrice\n\nNow let's take a look at our most important variable - the sale price. It turns out there is a long tail of outlying properties with high sale prices, which biases the mean to be much higher than the median."},{"metadata":{"trusted":true,"_uuid":"9f90d4f9f51e95fa7e0d3ba18c4f3b1dcd3ff342","collapsed":true},"cell_type":"code","source":"print(df_all.SalePrice.describe())\n\nplt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nsns.distplot(df_all.SalePrice.dropna() , fit=stats.norm);\nplt.subplot(1,2,2)\n_=stats.probplot(df_all.SalePrice.dropna(), plot=plt)","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"3f2e8f05ccab7c95d408dc6e2260bac2895a8baa"},"cell_type":"markdown","source":"## Log Transform SalePrice\n\nMany models make assumptions about having normally distributed variables, and the long tail above could hurt the performance of the model fitting later. Log transofrming SalePrice has the double benefit of significantly improving its normality, and being more relevant for the leaderboard evaluation metric (the root-mean-square-error of the log of the predicted sale price). "},{"metadata":{"trusted":true,"_uuid":"bc40dd296597b4c569a75e696ca0fe5691af241c","collapsed":true},"cell_type":"code","source":"#Log Transform SalePrice to improve normality\nsp = df_all.SalePrice\ndf_all.SalePrice = np.log(sp)\n\nprint(df_all.SalePrice.describe())\n\nplt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nsns.distplot(sp.dropna() , fit=stats.norm);\nplt.subplot(1,2,2)\n_=stats.probplot(sp.dropna(), plot=plt)","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"e2e30f55ecd02abbbd1fd5816d6714f6b42101ad"},"cell_type":"markdown","source":"## Identify Types of Features\n\nDifferent types of features will need to be treated differently when digging deeper in to the data. Here I identify three types of features:\n\n* **Numeric-discrete:** Numeric features with less than 13 unique values, such as month of the year, or the numeric scales created above.\n\n\n\n* **Numeric-continuous:** Numeric features with 13 or more unique values, such as areas, the year a property was built etc.\n\n\n\n* **Categorical:** The remaining non-numeric features."},{"metadata":{"trusted":true,"_uuid":"20ad7078bc55366f5c6b1d25b064f17db68d9a44","collapsed":true},"cell_type":"code","source":"# extract names of numeric columns\ndtypes = df_all.dtypes\ncols_numeric = dtypes[dtypes != object].index.tolist()\n\n# MSubClass should be treated as categorical\ncols_numeric.remove('MSSubClass')\n\n# choose any numeric column with less than 13 values to be\n# \"discrete\". 13 chosen to include months of the year.\n# other columns \"continuous\"\ncol_nunique = dict()\n\nfor col in cols_numeric:\n    col_nunique[col] = df_all[col].nunique()\n    \ncol_nunique = pd.Series(col_nunique)\n\ncols_discrete = col_nunique[col_nunique<13].index.tolist()\ncols_continuous = col_nunique[col_nunique>=13].index.tolist()\n\nprint(len(cols_numeric),'numeric columns, of which',\n      len(cols_continuous),'are continuous and',\n      len(cols_discrete),'are discrete.')","execution_count":18,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cbc2c28958d589a0fca6d5997be7177aa75ed39","collapsed":true},"cell_type":"code","source":"# extract names of categorical columns\ncols_categ = dtypes[~dtypes.index.isin(cols_numeric)].index.tolist()\n\nfor col in cols_categ:\n    df_all[col] = df_all[col].astype('category')\n    \nprint(len(cols_categ),'categorical columns.')","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"9cd83be46e0bdf575aea143b54ebe17336b46560"},"cell_type":"markdown","source":"<a id='distribFeats'></a>\n## Distribution of SalePrice in Categorical Variables\n\nStarting with the categorical variables, let's get a better idea of what the data looks like, and which features might be the most important for predicting sale price. Here I make use of seaborn's violinplot function, which nicely shows how the distribution and mean of the sale price changes for different categories within each feature.\n\nAt this point it's difficult to say much, other than there are definitely some values that have a large effect on sale price, e.g. properties in commercial zoning areas (`MSZoning == 'C(all)'`) have much lower sale prices on average. But there is also generally a large spread in sale prices within any single feature category value."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"3d3d1b7885b322e6a0981fb2fcf49c5891d26b25","collapsed":true},"cell_type":"code","source":"# plot categorical variables\nfcols = 3\nfrows = ceil(len(cols_categ)/fcols)\nplt.figure(figsize=(15,4*frows))\n\nfor i,col in enumerate(cols_categ):\n    plt.subplot(frows,fcols,i+1)\n    sns.violinplot(df_all[col],df_all['SalePrice'])","execution_count":20,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b4736bca870c6334c356039c0872d1067285fd5","collapsed":true},"cell_type":"code","source":"# few bigger plots for features with too many categories to see above\n\n#Neighbourhood\nplt.figure(figsize=(25,5))\nsns.violinplot(x='Neighborhood',y='SalePrice',data=df_all)\nplt.xticks(rotation=45);\n\n#Exterior1st\nplt.figure(figsize=(25,5))\nsns.violinplot(x='Exterior1st',y='SalePrice',data=df_all)\nplt.xticks(rotation=45);","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"c31d2f323e959603ea60fcec7c8492f3c0c45952"},"cell_type":"markdown","source":"<a id='sigcateg'></a>\n## Significance of Categorical Features for SalePrice\n\nTo get a better idea of the importance of each categorical variable for sale price prediction I perform an ANOVA test. I wrote my own function here (apart from obtaining the p-value at the end), as a learning exercise. You could do it directly with `scipy.stats.f_oneway` I believe.\n\nAlmost all the categorical features show a significant effect on sale price, with Neighborhood, Garage Type, MSubClass (type of dwelling) and Foudation apparently the most important."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"48a4abc2156b1afb28bf1587d2f97891e6a43240"},"cell_type":"code","source":"# anova test to check significance of variation in column 'group' vs. column 'value' \ndef anova(group,value):\n    # select columns of interest, and remove any rows with nan values\n    data = df_all[[group,value]]\n    data = data[~(data[group].isnull() | data[value].isnull())]\n    \n    # stats across all data\n    tot_groups = data[group].nunique() # no. of groups\n    len_data = len(data) # total sample size of houses (all groups)\n    mean_data = data[value].mean() # mean across all groups\n    df_betwn = tot_groups - 1 # degrees of freedom betwn grps\n    df_within = len_data - tot_groups # degrees of freedom within grps\n    \n    # per group stats\n    n_in_group = data.groupby(group)[value].count() # no. houses in group\n    mean_group = data.groupby(group)[value].mean() # mean value in this group\n    \n    # between-group variability\n    betwn_var = n_in_group*((mean_group - mean_data)**2)\n    betwn_var = float(betwn_var.sum())/df_betwn\n    \n    # within-group variability\n    within_var = 0\n    for grp in data[group].unique():\n        samples = data.loc[data[group]==grp, value]\n        within_var += ((samples-mean_group[grp])**2).sum()\n        \n    within_var = float(within_var)/df_within\n    \n    #F-test statistic\n    F = betwn_var/within_var\n    \n    # p-value\n    p = stats.f.sf(F, df_betwn, df_within)\n    \n    return p      ","execution_count":22,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"ba1bbda84a5e31f1d19cb12777f4b099b1747dd0","collapsed":true},"cell_type":"code","source":"# check significance of categorical variables on SalePrice\np_col = dict()\n\nfor col in cols_categ:\n    p_col[col] = anova(col,'SalePrice')\n    \npd.Series(p_col).sort_values()","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"b3c2b65546dc22245ae252ff2b2d805e8fc4daa2"},"cell_type":"markdown","source":"## Distribution of SalePrice in Discrete Numeric Features\n\nNow let's repeat the same process for the discrete numeric features. In this case some relationships are obvious, e.g. the sale price is strongly dependent on the overall quality (`OverallQual`) of a property. Alternatively, the month of sale seems to have no effect."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"ae9b38733b27a35ce9fba22c75b3b34acedd224f","collapsed":true},"cell_type":"code","source":"# figure parameters\nfcols = 3\nfrows = ceil(len(cols_discrete)/fcols)\nplt.figure(figsize=(15,4*frows))\n\nfor i,col in enumerate(cols_discrete):\n    plt.subplot(frows,fcols,i+1)\n    sns.violinplot(df_all[col],df_all['SalePrice'])","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"24b37d1cc51c60335cf84766de6fab457b6774b6"},"cell_type":"markdown","source":"<a id='sigdiscrete'></a>\n## Significance of Discrete Numeric Features for SalePrice\n\nPerforming ANOVA tests on the discrete variables gives sensible results based on what's observed above. The dependence on overall quality is so strong it registers a p-value of zero!"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"485783ef964794991d770a7d29b9e608494adc4c","collapsed":true},"cell_type":"code","source":"p_col = dict()\n\nfor col in cols_discrete:\n    p_col[col] = anova(col,'SalePrice')\n    \npd.Series(p_col).sort_values()","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"861a492d483bddadc91c95ac8b93d042c76cd116"},"cell_type":"markdown","source":"## Distribution of Continuous Variables and Effect on SalePrice\n\nFinally - the continuous variables. For these I revert back to scatter plots and standard histograms. Some strong relationships are immediately clear - for example between the `TotalAreaSF` feature created earlier and the sale price. The nuisance caused by zeros and/or outliers can again be seen by the poor quality regression lines in LotArea, for example."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"8e19e654d629364e4395599861a0fd6a079bd0e3","collapsed":true},"cell_type":"code","source":"# figure parameters\nfcols = 2\nfrows = len(cols_continuous)\nplt.figure(figsize=(5*fcols,4*frows))\n\ni=0\nfor col in cols_continuous:\n    i+=1\n    ax=plt.subplot(frows,fcols,i)\n    sns.regplot(x=col, y='SalePrice', data=df_all, ax=ax, \n                scatter_kws={'marker':'.','s':3,'alpha':0.3},\n                line_kws={'color':'k'});\n    plt.xlabel(col)\n    plt.ylabel('SalePrice')\n    \n    i+=1\n    ax=plt.subplot(frows,fcols,i)\n    sns.distplot(df_all[col].dropna() , fit=stats.norm)\n    plt.xlabel(col)","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"730ddc0765076a8189bc57e1c6d72e495a64a240"},"cell_type":"markdown","source":"<a id='correl'></a>\n## Correlation Between Numeric Features\n\nFor all the numeric features (discrete and continuous) let's check the correlation with sale price. I've used the Spearman correlation coefficient, which is more resilient to identifying non-linear relationships than Pearson. I also consider only the absolute value of the coefficient, as I'm interested in finding the strongest relationships, not whether they are positively/negatively correlated.\n\nThe most correlated features relate to the overall size and quality of the house. Having a garage also seems important, as well as the age of the property. The least correlated features tend to be those that appear in very few or almost all properties - for example nearly all houses have the full range of utilities, but very few have a pool. It's more surprising that the condition of a house does not seem very important for sale price - the distinction between that and the quality of a hosue isn't particularly clear to me."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"e87b066628850e7d3707909cd2f477cdaf012478","collapsed":true},"cell_type":"code","source":"# correlation between numeric variables\ndf_corr = df_all.loc[id_train, cols_numeric].corr(method='spearman').abs()\n\n# order columns and rows by correlation with SalePrice\ndf_corr = df_corr.sort_values('SalePrice',axis=0,ascending=False).sort_values('SalePrice',axis=1,ascending=False)\n\nprint(df_corr.SalePrice.head(20))\nprint('-----------------')\nprint(df_corr.SalePrice.tail(10))\n\nax=plt.figure(figsize=(20,16)).gca()\nsns.heatmap(df_corr,ax=ax,square=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07ef7539160038d2b9f7674ff81ba67f27289f6f"},"cell_type":"markdown","source":"## Collinearity\n\nAs can be seen in the heatmap above, many of the features are correlated with each other. Some of these are clear - for example the area of a garage and the number of cars it can hold are clearly related. This relationship is plotted below, although the distinction maybe isn't as clear as expected - there is a garage with capacity for 1 car that has an area bigger than many of the garages with capacity for 4 cars, for example.\n\nA lot of the features I created earlier, such as the dummy variables that indicate whether a feature is present or not, are natrually highly correlated with the features they represent.\n\nHaving collinear features can make it more difficult for our models to identify which parameters are truly important. Some of the features, like garage area, provide very little extra information and probably could be removed.  However, I have taken a lazy approach, kept all the features, and relied on the models to do a decent enough job at picking out which features to use.\n\nBut this is definitely something that could be looked at to improve, and removing some features may help some of the more complicated models get better results."},{"metadata":{"trusted":true,"_uuid":"e5558393c6fc473014351c18e446530135e82477","collapsed":true},"cell_type":"code","source":"sns.regplot(x='GarageCars',y='GarageArea',data=df_all)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a3d4bd6201b261380ff8dba44cad81b90f49072"},"cell_type":"markdown","source":"## Normalise Numeric Features\n\nAs we have features with very different scales (such as areas and counts), I normalise all the numeric features using a simple function I defined earlier (in the LotFrontage NaN values section - though again there are built-in functions in sklearn etc. that could do this) to lie between zero and one. This helps to prevent the model from being biased towards using certain features."},{"metadata":{"trusted":true,"_uuid":"a270195a82e42dd192be915db1695a6e79ba6036","collapsed":true},"cell_type":"code","source":"# normalise numeric columns\nscale_cols = [col for col in cols_numeric if col!='SalePrice']\n\ndf_all[scale_cols] = df_all[scale_cols].apply(scale_minmax,axis=0)\n\ndf_all[scale_cols].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8865c42ecd76b6e8a8ab9dbcb3207cce0ee07930"},"cell_type":"markdown","source":"<a id='transformFeats'></a>\n## Which Numeric Features are Candidates to be Transformed?\n\nLike I did for `SalePrice`, we can transform the continuous numeric features to try to improve their normality. For `SalePrice` I used a simple log transform, but here I try the more flexible Box-Cox transform, letting the `scipy.stats.boxcox` function optimise the parameters of the fit in each case.\n\nHere I plot the original and transformed distributions for all the continuous features to identify which transformations look reasonable.\n\nNB - I did try using BoxCox on `SalePrice` too, but it gives no improvement or is slightly worse than log transform. So I kept the log transform for `SalePrice` as it's easier to revert it in to the original dollar values.\n"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"dee4c86ce520991290d406a2cd8870cd51938777","collapsed":true},"cell_type":"code","source":"#Check effect of Box-Cox transforms on distributions of continuous variables\nfcols = 6\nfrows = len(cols_continuous)-1\nplt.figure(figsize=(4*fcols,4*frows))\ni=0\n\nfor var in cols_continuous:\n    if var!='SalePrice':\n        dat = df_all[[var, 'SalePrice']].dropna()\n        \n        i+=1\n        plt.subplot(frows,fcols,i)\n        sns.distplot(dat[var] , fit=stats.norm);\n        plt.title(var+' Original')\n        plt.xlabel('')\n        \n        i+=1\n        plt.subplot(frows,fcols,i)\n        _=stats.probplot(dat[var], plot=plt)\n        plt.title('skew='+'{:.4f}'.format(stats.skew(dat[var])))\n        plt.xlabel('')\n        plt.ylabel('')\n        \n        i+=1\n        plt.subplot(frows,fcols,i)\n        plt.plot(dat[var], dat['SalePrice'],'.',alpha=0.5)\n        plt.title('corr='+'{:.2f}'.format(np.corrcoef(dat[var], dat['SalePrice'])[0][1]))\n \n        i+=1\n        plt.subplot(frows,fcols,i)\n        trans_var, lambda_var = stats.boxcox(dat[var].dropna()+1)\n        trans_var = scale_minmax(trans_var)      \n        sns.distplot(trans_var , fit=stats.norm);\n        plt.title(var+' Tramsformed')\n        plt.xlabel('')\n        \n        i+=1\n        plt.subplot(frows,fcols,i)\n        _=stats.probplot(trans_var, plot=plt)\n        plt.title('skew='+'{:.4f}'.format(stats.skew(trans_var)))\n        plt.xlabel('')\n        plt.ylabel('')\n        \n        i+=1\n        plt.subplot(frows,fcols,i)\n        plt.plot(trans_var, dat['SalePrice'],'.',alpha=0.5)\n        plt.title('corr='+'{:.2f}'.format(np.corrcoef(trans_var,dat['SalePrice'])[0][1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4571956d170ef5ec207da70530a677120695241e"},"cell_type":"markdown","source":"## Box-Cox Transform Suitable Variables\n\nThe transformation doesn't help, and in some cases degrades, some of the variables, usually because of those pesky zeros again. For example, `2ndFlrSF` has many zeros for properties that don't have a second floor. The transformation in this case doesn't achieve much other than separating the zero and non-zero points by a greater margin.\n\nBy inspecting all the plots above I define a list of features for which the transform doesn't help. I transform and then re-normalise the remaining features."},{"metadata":{"trusted":true,"_uuid":"e6f46e4406ddb57c8cf9027fa01054462e62710d","collapsed":true},"cell_type":"code","source":"# variables not suitable for box-cox transformation based on above (usually due to excessive zeros)\ncols_notransform = ['2ndFlrSF','1stFlrFrac','2ndFlrFrac','StorageAreaSF',\n                    'EnclosedPorch','LowQualFinSF','MasVnrArea',\n                    'MiscVal','ScreenPorch','OpenPorchSF','WoodDeckSF','SalePrice',\n                    'BsmtGLQSF','BsmtALQSF','BsmtBLQSF','BsmtRecSF','BsmtLwQSF','BsmtUnfSF',\n                    'BsmtGLQFrac','BsmtALQFrac','BsmtBLQFrac','BsmtRecFrac','BsmtLwQFrac','BsmtUnfFrac']\n\ncols_transform = [col for col in cols_continuous if col not in cols_notransform]\n\n#transform remaining variables\nprint('Transforming',len(cols_transform),'columns:',cols_transform)\n\nfor col in cols_transform:   \n    # transform column\n    df_all.loc[:,col], _ = stats.boxcox(df_all.loc[:,col]+1)\n    \n    # renormalise column\n    df_all.loc[:,col] = scale_minmax(df_all.loc[:,col])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e63ab38d50b5d2fd59d5165acf456b2b7ffc4cf"},"cell_type":"markdown","source":"<a id='dummies'></a>\n## Prepare Data for Model Fitting\n\nAlmost time to start fitting some models, all that's left to do is to figure out what to do with the remaining categorical features. I decided to use one-hot encoding (creating a binary indicator for each possible feature value), which is very easy to obtain with the `pandas.DataFrame.get_dummies` function. This ends up increasing the number of variables in the dataset from about 100 to around 250.\n\nI did briefly try other methods, such as label encoding, but they seemed to give worse results. However, I suspect that approach might be more useful when adding feature interactions (see below)."},{"metadata":{"trusted":true,"_uuid":"8c316522f301b548ea66e28660c084cbc76881f6","collapsed":true},"cell_type":"code","source":"# select features, encode categoricals, create dataframe for model fitting\n\n# select which features to use (all for now)\nmodel_cols = df_all.columns\n\n# encode categoricals\ndf_model = pd.get_dummies(df_all[model_cols])\n\n# Rather than including Condition1 and Condition2, or Exterior1st and Exterior2nd,\n# combine the dummy variables (allowing 2 true values per property)\nif ('Condition1' in model_cols) and ('Condition2' in model_cols):\n    \n    cond_suffix = ['Artery','Feedr','Norm','PosA','PosN','RRAe','RRAn','RRNn']\n    \n    for suffix in cond_suffix:\n        col_cond1 = 'Condition1_'+suffix\n        col_cond2 = 'Condition2_'+suffix\n        \n        df_model[col_cond1] = df_model[col_cond1] | df_model[col_cond2]\n        df_model.drop(col_cond2,axis=1,inplace=True)\n        \nif ('Exterior1st' in model_cols) and ('Exterior2nd' in model_cols):\n    \n    # some different strings in Exterior1st and Exterior2nd for same type - rename columns to correct\n    df_model.rename(columns={'Exterior2nd_Wd Shng':'Exterior2nd_WdShing',\n                             'Exterior2nd_Brk Cmn':'Exterior2nd_BrkComm',\n                             'Exterior2nd_CmentBd':'Exterior2nd_CemntBd'},inplace=True)\n\n    ext_suffix = ['AsphShn','BrkComm','BrkFace','CBlock','CemntBd',\n                    'HdBoard','ImStucc','MetalSd','Plywood','Stone',\n                    'Stucco','VinylSd','Wd Sdng','WdShing','AsbShng']\n    \n    for suffix in ext_suffix:\n        col_cond1 = 'Exterior1st_'+suffix\n        col_cond2 = 'Exterior2nd_'+suffix\n        \n        df_model[col_cond1] = df_model[col_cond1] | df_model[col_cond2]\n        df_model.drop(col_cond2,axis=1,inplace=True)\n        \ndisplay(df_model.head())","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"fbc1769fa268bd13d68086c3b35a72b7b9c4a60e"},"cell_type":"code","source":"# function to get training samples\ndef get_training_data():\n    # extract training samples\n    df_train = df_model.loc[id_train]\n    \n    # split SalePrice and features\n    y = df_train.SalePrice\n    X = df_train.drop('SalePrice',axis=1)\n    \n    return X, y\n\n# extract test data (without SalePrice)\ndef get_test_data():\n    return df_model.loc[id_test].drop('SalePrice',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cb4eac29e20b045cd420f902414a93d25512c72"},"cell_type":"markdown","source":"<a id='outliers'></a>\n## Identify and Remove Outliers\n\nNow time to start fitting some models! First I fit a linear regression Ridge model, because it immediately gives good results with default values. My goal here is to use a straightforward model to identify any outlying points - properties with sale prices far away from what's expected given their features.\n\nThe function I define below fits the model and checks the residuals between the model's predicted sale prices and the true sale prices. The standard deviation of the residuals is calculated, and the 20 or so properties with residuals greater than three times this standard deviation are removed from the training data so they don't skew the parameters of the fitted models.\n\nJust Ridge with default values already gives a decent prediction, which is a promising sign."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"552a88e7943981be72da87dcf0ca48ec134a5e83"},"cell_type":"code","source":"# metric for evaluation\ndef rmse(y_true, y_pred):\n    diff = y_pred - y_true\n    sum_sq = sum(diff**2)    \n    n = len(y_pred)   \n    \n    return np.sqrt(sum_sq/n)\n\n# scorer to be used in sklearn model fitting\nrmse_scorer = make_scorer(rmse, greater_is_better=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"a6ee3bc828f3faae01a7ae57f11fc3c1f43d696a"},"cell_type":"code","source":"# function to detect outliers based on the predictions of a model\ndef find_outliers(model, X, y, sigma=3):\n\n    # predict y values using model\n    try:\n        y_pred = pd.Series(model.predict(X), index=y.index)\n    # if predicting fails, try fitting the model first\n    except:\n        model.fit(X,y)\n        y_pred = pd.Series(model.predict(X), index=y.index)\n        \n    # calculate residuals between the model prediction and true y values\n    resid = y - y_pred\n    mean_resid = resid.mean()\n    std_resid = resid.std()\n\n    # calculate z statistic, define outliers to be where |z|>sigma\n    z = (resid - mean_resid)/std_resid    \n    outliers = z[abs(z)>sigma].index\n    \n    # print and plot the results\n    print('R2=',model.score(X,y))\n    print('rmse=',rmse(y, y_pred))\n    print('---------------------------------------')\n\n    print('mean of residuals:',mean_resid)\n    print('std of residuals:',std_resid)\n    print('---------------------------------------')\n\n    print(len(outliers),'outliers:')\n    print(outliers.tolist())\n\n    plt.figure(figsize=(15,5))\n    ax_131 = plt.subplot(1,3,1)\n    plt.plot(y,y_pred,'.')\n    plt.plot(y.loc[outliers],y_pred.loc[outliers],'ro')\n    plt.legend(['Accepted','Outlier'])\n    plt.xlabel('y')\n    plt.ylabel('y_pred');\n\n    ax_132=plt.subplot(1,3,2)\n    plt.plot(y,y-y_pred,'.')\n    plt.plot(y.loc[outliers],y.loc[outliers]-y_pred.loc[outliers],'ro')\n    plt.legend(['Accepted','Outlier'])\n    plt.xlabel('y')\n    plt.ylabel('y - y_pred');\n\n    ax_133=plt.subplot(1,3,3)\n    z.plot.hist(bins=50,ax=ax_133)\n    z.loc[outliers].plot.hist(color='r',bins=50,ax=ax_133)\n    plt.legend(['Accepted','Outlier'])\n    plt.xlabel('z')\n    \n    plt.savefig('outliers.png')\n    \n    return outliers","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"d10d01239748c215c7ef38a18750c543186ad475","collapsed":true},"cell_type":"code","source":"# get training data\nX, y = get_training_data()\n\n# find and remove outliers using a Ridge model\noutliers = find_outliers(Ridge(), X, y)\n\n# permanently remove these outliers from the data\ndf_model = df_model.drop(outliers)\nid_train = id_train.drop(outliers)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0eb5dd6f47847e72be2fdf0cb7cd3c7cb5d238a5"},"cell_type":"markdown","source":"<a id='modelfit'></a>\n## Fit and Optimise Models\n\nTo avoid repeating code I create a function to perform the fitting process for each model type I try. I use k-fold cross-validation to reduce the chances of overfitting the training data - specifically 5-fold cross-validation repeated 5 times (another choice mainly to try another function I hadn't used before - `sklearn.model_selection.RepeatedKFold`). The parameters of each model are optimised using a grid search, and the function returns the best model found and some stats on the model performance.\n\nThe models I optimise are:\n\n**Linear Regression:**\n* `sklearn.linear_model.Ridge`\n* `sklearn.linear_model.Lasso`\n* `sklearn.linear_model.ElasticNet`\n\n**Support Vector Machines:**\n* `sklearn.svm.LinearSVR`\n* `sklearn.svm.SVR`\n\n**Nearest Neighbours:**\n* `sklearn.neighbors.KNearestNeighborsRegressor`\n\n**Tree Based:**\n* `sklearn.ensemble.RandomForestRegressor`\n* `sklearn.ensemble.GradientBoostingRegressor`\n* `xgboost.XGBRegressor`\n\nThere is some trial and error involved in finding the right parameter ranges to search for each model. The current ranges in the notebook contain the optimal values I found whilst being restictive enough to compute within a few minutes on my measly laptop."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"f73ea05eb159c169d37de94bf494e3eae59cfe1e"},"cell_type":"code","source":"def train_model(model, param_grid=[], X=[], y=[], \n                splits=5, repeats=5):\n\n    # get unmodified training data, unless data to use already specified\n    if len(y)==0:\n        X,y = get_training_data()\n    \n    # create cross-validation method\n    rkfold = RepeatedKFold(n_splits=splits, n_repeats=repeats)\n    \n    # perform a grid search if param_grid given\n    if len(param_grid)>0:\n        # setup grid search parameters\n        gsearch = GridSearchCV(model, param_grid, cv=rkfold,\n                               scoring=rmse_scorer,\n                               verbose=1, return_train_score=True)\n\n        # search the grid\n        gsearch.fit(X,y)\n\n        # extract best model from the grid\n        model = gsearch.best_estimator_        \n        best_idx = gsearch.best_index_\n\n        # get cv-scores for best model\n        grid_results = pd.DataFrame(gsearch.cv_results_)       \n        cv_mean = abs(grid_results.loc[best_idx,'mean_test_score'])\n        cv_std = grid_results.loc[best_idx,'std_test_score']\n\n    # no grid search, just cross-val score for given model    \n    else:\n        grid_results = []\n        cv_results = cross_val_score(model, X, y, scoring=rmse_scorer, cv=rkfold)\n        cv_mean = abs(np.mean(cv_results))\n        cv_std = np.std(cv_results)\n    \n    # combine mean and std cv-score in to a pandas series\n    cv_score = pd.Series({'mean':cv_mean,'std':cv_std})\n\n    # predict y using the fitted model\n    y_pred = model.predict(X)\n    \n    # print stats on model performance         \n    print('----------------------')\n    print(model)\n    print('----------------------')\n    print('score=',model.score(X,y))\n    print('rmse=',rmse(y, y_pred))\n    print('cross_val: mean=',cv_mean,', std=',cv_std)\n    \n    # residual plots\n    y_pred = pd.Series(y_pred,index=y.index)\n    resid = y - y_pred\n    mean_resid = resid.mean()\n    std_resid = resid.std()\n    z = (resid - mean_resid)/std_resid    \n    n_outliers = sum(abs(z)>3)\n    \n    plt.figure(figsize=(15,5))\n    ax_131 = plt.subplot(1,3,1)\n    plt.plot(y,y_pred,'.')\n    plt.xlabel('y')\n    plt.ylabel('y_pred');\n    plt.title('corr = {:.3f}'.format(np.corrcoef(y,y_pred)[0][1]))\n    ax_132=plt.subplot(1,3,2)\n    plt.plot(y,y-y_pred,'.')\n    plt.xlabel('y')\n    plt.ylabel('y - y_pred');\n    plt.title('std resid = {:.3f}'.format(std_resid))\n    \n    ax_133=plt.subplot(1,3,3)\n    z.plot.hist(bins=50,ax=ax_133)\n    plt.xlabel('z')\n    plt.title('{:.0f} samples with z>3'.format(n_outliers))\n\n    return model, cv_score, grid_results","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"2c5b345edb8923cf5c8ec27d16f5d83c84318322"},"cell_type":"code","source":"# places to store optimal models and scores\nopt_models = dict()\nscore_models = pd.DataFrame(columns=['mean','std'])\n\n# no. k-fold splits\nsplits=5\n# no. k-fold iterations\nrepeats=5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40c570e69427d65cadf96e841ac24db10bc56721"},"cell_type":"markdown","source":"## Linear Regression"},{"metadata":{"_uuid":"42519a1053a5602577f5e0867339878e29ddf642"},"cell_type":"markdown","source":"### Ridge"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"1a0d5c9177f9a1efbe35c7b2150a77b3a6b5a60b","collapsed":true},"cell_type":"code","source":"model = 'Ridge'\n\nopt_models[model] = Ridge()\nalph_range = np.arange(0.25,6,0.25)\nparam_grid = {'alpha': alph_range}\n\nopt_models[model],cv_score,grid_results = train_model(opt_models[model], param_grid=param_grid, \n                                              splits=splits, repeats=repeats)\n\ncv_score.name = model\nscore_models = score_models.append(cv_score)\n\nplt.figure()\nplt.errorbar(alph_range, abs(grid_results['mean_test_score']),\n             abs(grid_results['std_test_score'])/np.sqrt(splits*repeats))\nplt.xlabel('alpha')\nplt.ylabel('score');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8ebbb02efb54b9b04df26249fc431322db16942"},"cell_type":"markdown","source":"### Lasso"},{"metadata":{"trusted":true,"_uuid":"7ed534ad0b9d3373e1bc1fb54799e9cf13b68e52","collapsed":true},"cell_type":"code","source":"model = 'Lasso'\n\nopt_models[model] = Lasso()\nalph_range = np.arange(1e-4,1e-3,4e-5)\nparam_grid = {'alpha': alph_range}\n\nopt_models[model], cv_score, grid_results = train_model(opt_models[model], param_grid=param_grid, \n                                              splits=splits, repeats=repeats)\n\ncv_score.name = model\nscore_models = score_models.append(cv_score)\n\nplt.figure()\nplt.errorbar(alph_range, abs(grid_results['mean_test_score']),abs(grid_results['std_test_score'])/np.sqrt(splits*repeats))\nplt.xlabel('alpha')\nplt.ylabel('score');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6eb60b9cc3a0e9c1a6ef91d2406e89424860461"},"cell_type":"markdown","source":"### ElasticNet"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"a18764a3c38dde4ac79853a1c0e6dbb54e4799c4","collapsed":true},"cell_type":"code","source":"model ='ElasticNet'\nopt_models[model] = ElasticNet()\n\nparam_grid = {'alpha': np.arange(1e-4,1e-3,1e-4),\n              'l1_ratio': np.arange(0.1,1.0,0.1),\n              'max_iter':[100000]}\n\nopt_models[model], cv_score, grid_results = train_model(opt_models[model], param_grid=param_grid, \n                                              splits=splits, repeats=repeats)\n\ncv_score.name = model\nscore_models = score_models.append(cv_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7464947884ec92992254e8fbbf5331a8a536a4fd"},"cell_type":"markdown","source":"## SVM"},{"metadata":{"_uuid":"9ca83787ecfeb6238e7639bd00ce19d1e61da70e"},"cell_type":"markdown","source":"### Linear"},{"metadata":{"trusted":true,"_uuid":"b440da52ff98eef504b1a01ebb2d94d94a6c7aa1","collapsed":true},"cell_type":"code","source":"model='LinearSVR'\nopt_models[model] = LinearSVR()\n\ncrange = np.arange(0.1,1.0,0.1)\nparam_grid = {'C':crange,\n             'max_iter':[100000]}\n\nopt_models[model], cv_score, grid_results = train_model(opt_models[model], param_grid=param_grid, \n                                              splits=splits, repeats=repeats)\n\ncv_score.name = model\nscore_models = score_models.append(cv_score)\n\nplt.figure()\nplt.errorbar(crange, abs(grid_results['mean_test_score']),abs(grid_results['std_test_score'])/np.sqrt(splits*repeats))\nplt.xlabel('C')\nplt.ylabel('score');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6287a6898bcc44f0541a21ed61217a355defb26e"},"cell_type":"markdown","source":"### Non-Linear"},{"metadata":{"trusted":true,"_uuid":"f6d31326cccf0305b173566379bd3942941bce5a","collapsed":true},"cell_type":"code","source":"model ='SVR'\nopt_models[model] = SVR()\n\nparam_grid = {'C':np.arange(1,21,2),\n              'kernel':['poly','rbf','sigmoid'],\n              'gamma':['auto']}\n\nopt_models[model], cv_score, grid_results = train_model(opt_models[model], param_grid=param_grid, \n                                              splits=splits, repeats=1)\n\ncv_score.name = model\nscore_models = score_models.append(cv_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d771412b97ef73b55beca0560578ad2fbc2b687"},"cell_type":"markdown","source":"## KNeighbors"},{"metadata":{"trusted":true,"_uuid":"8e5183bb1eca6ad79f2e70bd04682ae21492c849","collapsed":true},"cell_type":"code","source":"model = 'KNeighbors'\nopt_models[model] = KNeighborsRegressor()\n\nparam_grid = {'n_neighbors':np.arange(3,11,1)}\n\nopt_models[model], cv_score, grid_results = train_model(opt_models[model], param_grid=param_grid, \n                                              splits=splits, repeats=1)\n\ncv_score.name = model\nscore_models = score_models.append(cv_score)\n\nplt.figure()\nplt.errorbar(np.arange(3,11,1), abs(grid_results['mean_test_score']),abs(grid_results['std_test_score'])/np.sqrt(splits*1))\nplt.xlabel('n_neighbors')\nplt.ylabel('score');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"128ec2165aa763a1406d4f83b0f615eefc864753"},"cell_type":"markdown","source":"## Tree-Based"},{"metadata":{"_uuid":"eea836d574eb8a1cd6a6686686e476bc94196c5b"},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true,"_uuid":"093cc86bd360a1059be85952c70785ab0d54a432","collapsed":true},"cell_type":"code","source":"model = 'RandomForest'\nopt_models[model] = RandomForestRegressor()\n\nparam_grid = {'n_estimators':[100,150,200],\n              'max_features':[25,50,75],\n              'min_samples_split':[2,4,6]}\n\nopt_models[model], cv_score, grid_results = train_model(opt_models[model], param_grid=param_grid, \n                                              splits=5, repeats=1)\n\ncv_score.name = model\nscore_models = score_models.append(cv_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"138744d1978adf19e3fa2bf574c271fd06ae17f7"},"cell_type":"markdown","source":"### Gradient Boosting"},{"metadata":{"trusted":true,"_uuid":"28381867ec4e5c4e0e83a3d27fa826cc8e87b5b0","collapsed":true},"cell_type":"code","source":"model = 'GradientBoosting'\nopt_models[model] = GradientBoostingRegressor()\n\nparam_grid = {'n_estimators':[150,250,350],\n              'max_depth':[1,2,3],\n              'min_samples_split':[5,6,7]}\n\nopt_models[model], cv_score, grid_results = train_model(opt_models[model], param_grid=param_grid, \n                                              splits=splits, repeats=1)\n\ncv_score.name = model\nscore_models = score_models.append(cv_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30d748351abb8cad9b452483568a917154d8a23a"},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true,"_uuid":"9d7c905d2d8b99dda4c00e58f5041a3a59ce5b6c","collapsed":true},"cell_type":"code","source":"model = 'XGB'\nopt_models[model] = XGBRegressor()\n\nparam_grid = {'n_estimators':[100,200,300,400,500],\n              'max_depth':[1,2,3],\n             }\n\nopt_models[model], cv_score,grid_results = train_model(opt_models[model], param_grid=param_grid, \n                                              splits=splits, repeats=1)\n\ncv_score.name = model\nscore_models = score_models.append(cv_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e228e7179fb116ec8d9b131d424c4556fec8109"},"cell_type":"markdown","source":"## Compare Models\n\nRoughly speaking the linear regression models perform best, then support vector machines, then the tree-based ensembles and KNeighbors brings up the rear. The worst performing models, `KNeighbors` and `RandomForest`, have some correlation between the residuals and the sale price, which indicates there are relationships in the features those models haven't been able to identify as well as the others.\n\nMost of the features in the data have a close to linear relationship with sale price (at least the most important ones), so maybe it's not so surprising to see linear regression on top, even if they are the simplest, most restrictive models. The feature engineering I did was also with the limitations of linear regression in mind. However, I have seen in other people's notebooks that `XGBoost` tends to be better than, or at least similar to, the linear models. I don't have an explanation for why that isn't the case for me. "},{"metadata":{"trusted":true,"_uuid":"a6086a4bba91dc9599cd849ec53d53513b4d5c80","collapsed":true},"cell_type":"code","source":"X,y = get_training_data()\n\nY_models = pd.DataFrame(index=y.index)    \n\nfor key, model in opt_models.items():\n    Y_models[key] = model.predict(X)\n\ncorr_models = pd.DataFrame(Y_models).corr()\n\nscore_models = score_models.sort_values(by='mean')\n\ndisplay(score_models)\nscore_models['mean'].plot.barh()\nplt.xlabel('score')\n\nplt.figure()\nsns.heatmap(corr_models)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"666468e4a68137f1bf6e4b2677df308b0be88025"},"cell_type":"markdown","source":"## Best Model: ElasticNet\n\nElasticNet and Lasso give very similar results, sometimes Lasso gives a better cross-validation score, sometimes ElasticNet. ElasticNet tended to give me a slightly better leaderboard score so I stuck with that for my final model.\n\nBelow is a summary of the coefficients of the optimised ElasticNet model. About half the features are removed (have zero coefficient), most being dummy variables for certain categorical feature values. Some of the new features I created end up being removed, but others appear amongst the most important terms - such as `TotalAreaSF` and `BsmtGLQSF`. There are no big surprises - the features that end up appearing here are generally the same ones that stuck out in the exploratory data analysis earlier."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"6259be150cd98e8a60a92dd0f601eab332565633","collapsed":true},"cell_type":"code","source":"# get coefficients from previously optimised ElasticNet model\nen_coefs = pd.Series(opt_models['ElasticNet'].coef_,index=X.columns)\n\nplt.figure(figsize=(8,20))\nen_coefs[en_coefs.abs()>0.02].sort_values().plot.barh()\nplt.title('Coefficients with magnitude greater than 0.02')\n\nprint('---------------------------------------')\nprint(sum(en_coefs==0),'zero coefficients')\nprint(sum(en_coefs!=0),'non-zero coefficients')\nprint('---------------------------------------')\nprint('Intercept: ',opt_models['ElasticNet'].intercept_)\nprint('---------------------------------------')\nprint('Top 20 contributers to increased price:')\nprint('---------------------------------------')\nprint(en_coefs.sort_values(ascending=False).head(20))\nprint('---------------------------------------')\nprint('Top 10 contributers to decreased price:')\nprint('---------------------------------------')\nprint(en_coefs.sort_values(ascending=True).head(10))\nprint('---------------------------------------')\nprint('Zero coefficients:')\nprint('---------------------------------------')\nprint(en_coefs[en_coefs==0].index.sort_values().tolist())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64ab393f3fced818f6fbe9e6a257c0cdc4c4cbd0"},"cell_type":"markdown","source":"<a id='interactions'></a>\n## Find useful interactions\n\nAt this point I'm hovering somewhere between 0.117 and 0.118 on the leaderboard. I wanted to try to do a bit better and my attempts at ensemble/stacked models ended up giving worse scores (probbaly something I need to learn about), so I decided to try adding some interaction terms to the model, i.e. one feature multiplied by another. Intuitively it seems like these could be important for determining sale price, a small property finished to a high quality may be worth as much as a larger property that needs a lot of work doing to it, for example.\n\nIncluding all polynomial terms up to 2nd order ends up giving more than 30,000 features, far more than the number of training examples and unlikely to give good results if we try to fit a model using that. Instead I decided to only include features that meet these criteria:\n\n* Only include interaction terms (e.g. `TotalAreaSF*OverallQual`), exclude squared terms (e.g. `TotalAreaSF*TotalAreaSF`)\n* Only include features with non-zero coefficients in the optimised `ElasticNet` model above.\n* Only include features with more than 2 unique values, i.e. exclude all binary features and dummy variables.\n\nThis reduces the number of features in the new model to around 700. Excluding the dummy variables unfortunately means interactions with categorical variables like `Neighborhood` are not included. I suspect some of these could be important, but to include them in the model an approach other than one-hot encoding should probably be used, so you don't end up with thousands of columns that are ones and zeros multiplied together.\n\nThe code below fits a `Lasso` model to the new training data with interactions included, and prints some stats about the resulting coefficients."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"94b4e2f2b84512b20937d244cb8bd30febf43b66","collapsed":true},"cell_type":"code","source":"X,y=get_training_data()\n\n#restrict to non-binary columns with non-zero coefficients only\nselect_cols = [col for col in X.columns if X[col].nunique()>2 and en_coefs[col]!=0]\nX = X[select_cols]\n\n# add interaction terms (x1*x2, but not x1**2) for all remaining features\npoly = PolynomialFeatures(interaction_only=True)\nX_poly = poly.fit_transform(X,y)\nX_poly = pd.DataFrame(X_poly,index=y.index)\n\n# save info on which features contribute to each term in X_poly\npowers = pd.DataFrame(poly.powers_,columns=X.columns)\n\ns = np.where(powers>0, pd.Series(X.columns)+', ', '')\npoly_terms = pd.Series([''.join(x).strip() for x in s])\n\n# fit a new model with the interaction terms\nalph_range = np.arange(5e-5,2e-4,2e-5)\nparam_grid = {'alpha': np.arange(1e-4,1e-3,1e-4),\n              'max_iter':[100000]}\n\nmodel,_,_ = train_model(Lasso(), X=X_poly,y=y,param_grid=param_grid, \n                                              splits=splits, repeats=1)\n\npoly_coefs = pd.Series(model.coef_)\n\nprint('------------------------')\nprint(sum(poly_coefs==0),'zero coefficients')\nprint(sum(poly_coefs!=0),'non-zero coefficients')\nprint(len(poly_coefs[(powers.sum(axis=1)==2) & (poly_coefs>0)]),'non-zero interaction terms.')\nprint('------------------------')\nprint('Features with largest coefficients:')\nprint('------------------------')\nprint(poly_terms[poly_coefs.abs().sort_values(ascending=False).index[:30]])\nprint('------------------------')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4300ae95ba475e26258ad38c8378dac9586dafd"},"cell_type":"markdown","source":"## Refit with Important Interactions\n\nUsing the model above directly ends up giving a worse cross-validation score, bah! So I had to restrict myself to an even more limited range of interaction terms. \n\nAbove we can see that the number of non-zero coefficients in the new fitted model is actually about the same as before without the interaction terms - the vast majority of the new interactions I added end up being removed from the model. However, more promisingly about half of the non-zero coefficients correspond to interactions, so about 60 of the original features have been replaced in the model by interactions. And 20 out of the 30 largest coefficients are interactions! Some of the interactions that are important make sense, such as `BsmtQual*OverallQual`, or `KitchenQual*TotalAreaSF`. Others maybe not so much, like `Fireplaces*YearBuilt` which is now the 3rd most important feature!\n\nI decided to create a new model including the interactions with the top 20 largest (magnitude) coefficients, irrespective of whether they were interactions that seemed logical to me. The code below adds these interactions to the train and test data, then fits a new `ElasticNet` model."},{"metadata":{"trusted":true,"_uuid":"40697a9f398a51adff8a23a4a2641e26d3689801","collapsed":true},"cell_type":"code","source":"# sort coefficients by magnitude, and calculate no. features that contribute\n# to the polynomial term that coefficient represents.\npoly_coef_nterms = powers.loc[poly_coefs.abs().sort_values(ascending=False).index].sum(axis=1)\n\n# extract n_ints top interactions (interactions have 2 non-zero features in the powers vector)\nn_ints = 20\ntop_n_int_idx = poly_coef_nterms[poly_coef_nterms==2].head(n_ints).index\n\n# create a column for each of the top n_ints interactions in df_model\nfor idx in top_n_int_idx:\n    # extract names of columns to multiply\n    int_terms = powers.loc[idx]\n    int_terms = int_terms[int_terms==1].index    \n    term1 = int_terms[0]\n    term2 = int_terms[1]\n    \n    # create interaction column\n    df_model[term1+'_x_'+term2] = scale_minmax(df_model[term1]*df_model[term2])\n\n# have a look at the new columns\ndisplay(df_model[df_model.columns[df_model.columns.str.contains('_x_')]].head(5))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"0f81576d23ac910c3e0499040f6b66d46e4e4e28","collapsed":true},"cell_type":"code","source":"# fit a new model including the added interaction terms\nen_int = ElasticNet()\nalph_range = np.arange(5e-5,5e-4,2e-5)\nparam_grid = {'alpha': np.arange(1e-4,1e-3,1e-4),\n              'l1_ratio': np.arange(0.1,1.0,0.1),\n              'max_iter':[100000]}\n\nen_int, cv_score, grid_results = train_model(en_int, param_grid=param_grid, \n                                              splits=splits, repeats=repeats)\n\nX,y=get_training_data()\nen_int_coef = pd.Series(en_int.coef_,index=X.columns)\nprint('------------------------')\nprint(sum(en_int_coef==0),'zero coefficients')\nprint(sum(en_int_coef!=0),'non-zero coefficients')\nprint('--------------------------')\nprint('Interaction Coefficients:')\nprint('--------------------------')\nprint(en_int_coef[en_int_coef.index.str.contains('_x_')].sort_values(ascending=False))\nprint('---------------------------------------')\nprint('Top 10 contributers to increased price:')\nprint('---------------------------------------')\nprint(en_int_coef.sort_values(ascending=False).head(10))\nprint('---------------------------------------')\nprint('Top 5 contributers to decreased price:')\nprint('---------------------------------------')\nprint(en_int_coef.sort_values(ascending=True).head(5))\nprint('---------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d196fcf78b17b190af3dcb0ff5aba6cb6e946fc9"},"cell_type":"markdown","source":"## Predict SalePrice for the Test Data\n\nThe `ElasticNet` model including the top 20 interactions gives a better cross-validation score than the original models, and this is the final model I used. \n\nAll that's left is to predict the sale prices for the test data, create the submission file, and submit it to kaggle. The last time I submitted the notebook it gave a score of 0.11571. This varies slightly depending on the random cross-validation folds chosen and so on - the best I achieved was 0.11553."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"eb55e4caa5baf2b3044d571ea427f943eb53d59d"},"cell_type":"code","source":"def predict_test(model):\n    # get test data\n    X_test = get_test_data()\n\n    # predict SalePrices\n    y_test = model.predict(X_test)\n\n    #revert log transformation\n    y_test = np.exp(y_test)\n\n    # make a data frame suitable for creating submission files0.11571\n    y_test = pd.DataFrame(y_test,index=X_test.index)\n    y_test.columns = ['SalePrice']\n    \n    return y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d396981b3b01faf9a3ba4636ffce59cf81121bf6","collapsed":true},"cell_type":"code","source":"# predict sale prices\ny_test = predict_test(en_int)\n\n# make submission file\ny_test.to_csv('submission.csv')\n\n# overview of what sale prices were predicted\ny_test.plot.hist(bins=20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc48cf9d0da96171247969327a230cb486cebd7f"},"cell_type":"markdown","source":"## Thanks!\n\nThanks for reading and I hope this notebook is useful for someone getting started with this dataset, although there are many great kernels already out there. Any questions, comments or suggestions would be much appreciated!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}