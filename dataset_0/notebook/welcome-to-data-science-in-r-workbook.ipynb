{"cells":[{"metadata":{"_uuid":"a971bebcf83527c95ed740762af364f83ade2c9e","_cell_guid":"0d8c197b-8a10-47d2-9a46-1b8593a34353"},"cell_type":"markdown","source":"This is the workbook that accompanies the \"[Welcome to Data Science in R](https://www.kaggle.com/rtatman/welcome-to-data-science-in-r)\" lesson. You'll probably find it easier to complete the exercises if you read & follow the lesson itself. :)\n\n**How do I run the code in this notebook?** You can write and run code by forking the notebook (click the blue \"Fork Notebook\" button in the header). This will open the notebook editor and create a private copy of this notebook that you can work in. In the notebook editor, you can write code in any code cell (the ones with the grey background) and run the code by either 1) clicking in the code cell and then hitting CTRL + ENTER or 2) clicking in the code cell and the clicking on the white \"play\" arrow to the left of the cell. \n\n**How do I save my work?** Any changes you make are saved automatically as you work. You can run all the code in your notebook and save a static version by hitting the blue \"Commit & Run\" button in the upper right hand corner of the editor. \n\n**How can I find my notebook again later?** The easiest way is to go to your user profile (https://www.kaggle.com/replace-this-with-your-username), then click on the \"Kernels\" tab. All of your kernels will be under the \"Your Work\" tab, and all the kernels you've upvoted will be under the \"Favorites\" tab.\n___"},{"metadata":{"collapsed":true,"_uuid":"f8a653c6ff1e6603b9659c5b159e8f23418b4ed2","_cell_guid":"b01d66ae-dcf1-4988-a373-20f12be5361d"},"cell_type":"markdown","source":"# Table of Contents\n\n* [Starting your machine learning project](#Starting-your-machine-learning-project)\n* [Selecting and filtering data with the Tidyverse](#Selecting-and-filtering-data-with-the-Tidyverse)\n* [Running your first model](#Running-your-first-model)\n* [How do we know if our model is good?](#How-do-we-know-if-our-model-is-good?)\n* [Underfitting/overfitting and improving your model](#Underfitting/overfitting-and-improving-your-model)\n* [A different type of model: Random forests](#A-different-type-of-model:-Random-forests)"},{"metadata":{"_uuid":"153ac22e055ac87d77f0fb7665ac383ae1ab7963","_cell_guid":"1bff1b4b-ec7f-4a44-ad18-c73de01a9bc2"},"cell_type":"markdown","source":"# Starting your machine learning project\n\nI've started off by reading your data in for you. Make sure you run the first cell before you try to run any others!"},{"metadata":{"_kg_hide-output":true,"_uuid":"2811b7fc8048699c2f1c09cc7ff29ced9861b7d7","trusted":false,"_cell_guid":"a9d5d654-8be4-440c-bc35-b7f905e7ab90"},"cell_type":"code","source":"# load in the tidyverse package\nlibrary(tidyverse)\n\n# read the data and store data in a tibble\niowa_data <- read_csv(\"../input/train.csv\") \n\n# make sure Condition1 is a factor & not a char\niowa_data$Condition1 <- as.factor(iowa_data$Condition1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70fa734138e390dcc851e77c2a61b72c69856895","trusted":false,"_cell_guid":"b940681b-1765-4dc9-9984-486c29297ccc"},"cell_type":"code","source":"# Your turn: summarize the iowa_data dataframe\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8556fd1de365be8de6e08b9a30edae29f1664ad0","_cell_guid":"fc7aaf1c-1d1c-4dcb-9475-b4e7dcbcb183"},"cell_type":"markdown","source":"# Running your first model"},{"metadata":{"_uuid":"22a0b222c529e0fa3b438e38c3d57493b1811185","_cell_guid":"eaaa6d37-895e-4fd9-82e1-d685126196d0"},"cell_type":"markdown","source":"Now it's time for you to define and fit a model for your data.\n1. Select the target variable you want to predict. You can get a list of the columns in a data frame using the function col_names(), which is done for you in the cell below.\n2. Fit a model that can predict your target variable using the following predictors: \n    * LotArea\n    * YearBuilt\n    * Condition1 (how close to the main road the house is)\n    * FullBath\n    * BedroomAbvGr\n    * TotRmsAbvGrd\n\n3. Make a few predictions with the predict() function and print them out.\n4. Optional: Plot the decision "},{"metadata":{"_uuid":"deb03ece4ecf151a9ea33475fb65f4c2924e1d20","trusted":false,"_cell_guid":"12a771f8-530b-4262-a050-67c5b66ab89f"},"cell_type":"code","source":"# Your turn: build a model to predict housing prices in Iowa\n\n# library for building decision trees\nlibrary(rpart)\n\n# print a list of the column names\nnames(iowa_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f4386189b671bec04378bf88df09a7d3a911cf7","_cell_guid":"cee9bd66-bcfa-47dc-aaa1-221c1f94fc9e"},"cell_type":"markdown","source":"# How do we know if our model is good?"},{"metadata":{"_uuid":"1ebcca918bf20e0c88ddaff13b752312c3ccd139","trusted":false,"_cell_guid":"308e9306-bf0f-458f-9af3-fe09f1f6bd39"},"cell_type":"code","source":"# Your turn: split your training data into test & training sets\n\n# Fit a new model to your training set...\n\n\n# and evaluate it on your test set. Did the error get larger or smaller?\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64ce0bc31cdd230bccd6916bfe0f57a7a412cc41","_cell_guid":"85be1bd5-4ad7-49a2-acbd-98cda7235c93"},"cell_type":"markdown","source":"# Underfitting/overfitting and improving your model\n\nUse a for loop that tries different values of *maxdepth* and calls the *get_mae* function on each to find the ideal number of leaves for your Iowa data."},{"metadata":{"_uuid":"bf574e4ca0172f5206ee43d47c426a0a2aee9e0b","trusted":false,"_cell_guid":"44dd2d51-198d-4214-8f15-62d9d51a1758"},"cell_type":"code","source":"# a function to get the maximum average error for a given max depth. You should pass in\n# the target as the name of the target column and the predictors as vector where\n# each item in the vector is the name of the column\nget_mae <- function(maxdepth, target, predictors, training_data, testing_data){\n    \n    # turn the predictors & target into a formula to pass to rpart()\n    predictors <- paste(predictors, collapse=\"+\")\n    formula <- as.formula(paste(target,\"~\",predictors,sep = \"\"))\n    \n    # build our model\n    model <- rpart(formula, data = training_data,\n                   control = rpart.control(maxdepth = maxdepth))\n    # get the mae\n    mae <- mae(model, testing_data)\n    return(mae)\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b6b1a4f4f4972032513ed40555151ca4cc00ae6","trusted":false,"_cell_guid":"5750aa29-7286-4268-b0e4-0fd84ee28f1d"},"cell_type":"code","source":"# Your turn: use the get_mae function to find the maxdepth that leads to the \n# lowest mean average error for this dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"993700d16b6bb6ee2fb0fc2b18b271cfb552340e","_cell_guid":"6b869499-efcd-49f6-bb31-d1c1ddf42a8f"},"cell_type":"markdown","source":"# A different type of model: Random forests\n\nNow it's your turn to fit a randomForest on your data. You're going to need to read in the randomForest library to do this, so be sure to run the first cell before you try to make a call to the randomForest() function or you'll get an error!"},{"metadata":{"_uuid":"97083be706192a2de0bb98825459d64a39af6386","trusted":false,"_cell_guid":"5e38dd07-f28f-4bf9-a38a-394e1456fe9b"},"cell_type":"code","source":"# read in the library we'll use for random forests\nlibrary(randomForest)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd9bbe6ef3cf27c3aa583c1d2b761dffe31a4444","trusted":false,"_cell_guid":"632878f0-db03-4176-8e4e-dcd9639bc813"},"cell_type":"code","source":"# Your turn: Train a random forest using the same features as you used\n# to train your original decision tree.\n\n\n# Check out the MAE. Did you see an improvement over your original model?","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}