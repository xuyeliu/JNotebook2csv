{"cells":[{"metadata":{"_cell_guid":"9f988bd9-2a50-4e82-ba89-3994eb26fe98","_uuid":"10ee3e0d903342ad67d71ab1049e8f106339f1aa"},"cell_type":"markdown","source":"# First (baby) steps\n*RocÃ­o Byron, Jan. 2018*\n\nThe goal of this notebook is to test out how far one can get with the simplest tools available: basic feature engineering and linear regression modeling. And on the way, learn how to work on Jupyter Notebook.\n\n***\n## Inspirations\n\nThere are hundreds of kernels on this competition, and I have probably read half of them at some point or another. Two kernels, however, have inspired me the most:\n\n1. [Stacked Regressions : Top 4% on LeaderBoard](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard) by Serigne\n2. The ubiquous [Comprehensive data exploration with Python](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python) by Pedro Marcelino\n\n## Outline\n\n1. Preprocessing\n    1. Understanding the problem and the data available\n    2. Normality and skewness\n    3. Missing values\n    4. Dummy encoding\n    5. Rescaling\n2. Regression\n    1. Linear regression\n    2. L1 regularisation\n    3. L2 regularisation\n    4. ElasticNet regularisation"},{"metadata":{"collapsed":true,"_cell_guid":"59e13299-b391-4f7d-85f2-030bd1b409ac","_uuid":"ab0b9bd88b831c1b52243a76e63e70917a6f433b","trusted":false},"cell_type":"code","source":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"66092885-1352-44f8-8578-7d7797842c98","_uuid":"351e4d0f774d7eee6d53a173f2d39b8ab376ed1b"},"cell_type":"markdown","source":"# 1. Preprocessing \n\nThe main goal here is to get rid of (very) spurious data points and prepare the dataset for learning. \n\nThis is a very delicate process: go to far, and you will be introducing bias in your data. Go to short, and you will be introducing rubbish in your learning process."},{"metadata":{"_cell_guid":"3699ea29-8ee9-43b0-b636-43c684e01cb6","_uuid":"1495af61e2ec55b4fd3dc6b640cd02ab57206bdd"},"cell_type":"markdown","source":"## A. The dataset"},{"metadata":{"_kg_hide-input":false,"collapsed":true,"_uuid":"acffc470134e6f08fe8fd70e3dfd23918333b77d","_kg_hide-output":false,"_cell_guid":"2ae2c5d8-9923-49c7-b298-2942407632da","scrolled":true,"trusted":false},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntestID = test['Id']\n\ndata = pd.concat([train.drop('SalePrice', axis=1), test], keys=['train', 'test'])\ndata.drop(['Id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"26a8cc23-63d5-446c-b3bf-d79753035a67","_uuid":"08feb49b924c19c74d0e595e461d46a424b9065e","trusted":false,"collapsed":true},"cell_type":"code","source":"data.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"77018d72-9158-400e-8a91-e1b0e1614170","_uuid":"8b68ff996ec13dfa5f49c80dd1cbdddd81d10bea"},"cell_type":"markdown","source":"### Sanity check\n\nBefore we go on and process this data, we need to be sure it actually makes sense. There are three \"low hanging fruits\" in this sense:\n- Features that represent years should not go take values larger than 2018\n- Areas, distances and prices should not take negative values\n- Months should be between 1 and 12"},{"metadata":{"collapsed":true,"_cell_guid":"f83a3558-b1ad-487b-aa02-072660964c77","_uuid":"8d435ea95f27e843e629e0cba2b6f56806134c19","trusted":false},"cell_type":"code","source":"years = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold']\nmetrics = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n         '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', \n         'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"20cb3de3-c4cd-4931-b490-56f9f3e5bd02","_uuid":"155ed62b4053a394661590e9f1917a7f8f212985","trusted":false,"collapsed":true},"cell_type":"code","source":"data[years].max()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"51278a6e-c3b0-4c7f-a5a4-6d612642526f","_uuid":"bab469b8b5403db0062aa531e1627eabf31de7a6","trusted":false,"collapsed":true},"cell_type":"code","source":"mask = (data[years] > 2018).any(axis=1) # take any index with a twisted year value\ndata[mask]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c428db36-9ddd-4c5c-8cda-979b6bafa487","_uuid":"8db7871edf83ee8f06ded37cd271101ea7019698","trusted":false},"cell_type":"code","source":"data.loc[mask, 'GarageYrBlt'] = data[mask]['YearBuilt']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"072a2b13-aedc-47eb-830f-156bb8a5c92d","_uuid":"cec91c9733fa9b0790ad8979d230667bf2f64adc","trusted":false,"collapsed":true},"cell_type":"code","source":"mask = (data[metrics] < 0).any(axis=1)\ndata[mask]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a373bcd6-6262-43c8-b023-21b30f9dd1f7","scrolled":false,"_uuid":"c8cb0447fe94ae5f2e66b490b79134e02c078ccd","trusted":false,"collapsed":true},"cell_type":"code","source":"mask = ((data['MoSold'] > 12) | (data['MoSold'] < 1))\ndata[mask]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2beed72c-8e1b-4f19-9fc6-d88b90455c66","_uuid":"2a9b13f669b1d188742ab8683c389ab4c3d3c3e4"},"cell_type":"markdown","source":"### Data types\n\nIn terms of data type, there are four big groups:\n\n1. Continuous numerical features: lengths, areas, prices\n2. Discrete numerical features: numerical scores, number of bedrooms, years; they support order and arithmetic operations, so they can be treated as numerical\n3. Ordinal categorical features: features with qualitative scores (such as 'Excellent' or 'Slightly Irregular'); They support ordering ('Gentle slope' < 'Severe slope') but not arithmetic operations (how much is 'Sever slope' - 'Gentle slope'?)\n4. Purely categorical features: a few examples are 'MSSubClass' or 'SaleType'\n\nAfter some trial and error, I decide to separate numerical (both continuous and discrete) from categorical.\n\nThe pros:\n- We keep the relationship between discrete numerical features\n- We end up with less features, hence less risk of overfitting\n\nThe cons: \n- We add some arbitrariness (if a house is an \"8\" is it really twice as better than a house that scores a 4?)"},{"metadata":{"collapsed":true,"_cell_guid":"0e47f943-6463-4cda-80f8-d528522bcb23","scrolled":true,"_uuid":"75687ed4b15679c9c5d559da5713700fc9558315","trusted":false},"cell_type":"code","source":"# Numerical features\nnum_feats = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', \n             'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'ExterQual', 'ExterCond', \n             'BsmtQual', 'BsmtCond', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', \n             'HeatingQC', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n             'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n             'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n             'Fireplaces', 'FireplaceQu', 'GarageYrBlt',\n             'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond',\n             'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n             'ScreenPorch', 'PoolArea', 'PoolQC', 'MiscVal',\n             'YrSold']    \n\n# We need to convert literal grades to a numerical scale\ngrades = ['OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',\n          'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\nliteral = ['Ex', 'Gd', 'TA', 'Fa', 'Po']\nnum = [9, 7, 5, 3, 2]\nG = dict(zip(literal, num))\n\ndata[grades] = data[grades].replace(G)\n\n# Categorical features: everything that is not 'numerical'\ncat_feats = data.drop(num_feats, axis=1).columns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"290a0fc6-3073-4ef0-ae42-ef7850c07e7f","_uuid":"2028d41bebaf308ca870a59d1c40f0e19367441e","trusted":false,"collapsed":true},"cell_type":"code","source":"cat_feats","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3b98baff-ac23-452f-acde-38562d0efc33","_uuid":"324ec564a8c2eb8909b3a6c2af06a3df9da939c1"},"cell_type":"markdown","source":"## B. Normality and skewness\n\nMany regression models are more comfortable with normally distributed variables (or at least something close to it). \n\nWe will, however, skip the discrete numerical features because:\n- The results will be more readable this way\n- Most of the discrete numerical features only take a few different values, so hoping for normality is a waste of time"},{"metadata":{"collapsed":true,"_cell_guid":"61bdb13d-ee69-423a-b620-66ee5ed37057","_uuid":"ea50ec20e5eeec249cebafbd03fba0facbec6c41","trusted":false},"cell_type":"code","source":"#log transform the target:\nprice = np.log1p(train['SalePrice'])\n\n#log transform skewed continuous numerical features:\nskewed_feats = data.loc['train'][metrics].apply(lambda x: x.skew(skipna=True)) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\ndata[skewed_feats] = np.log1p(data[skewed_feats])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1ea427ea-16da-4f4d-a041-8b3471bf6684","_uuid":"09d895336f35c69d530f10acb75bf76440b892fe"},"cell_type":"markdown","source":"## C. Missing values\n\nThere are two main types of NaN here:\n* Missing values: just some of the values that where not recorded (usually a small number of them)\n* Missing feature in the house: such as when there is no basement or no garage"},{"metadata":{"_cell_guid":"d2015478-fcb7-42e9-aac3-cc82cc1a13ef","scrolled":false,"_uuid":"2da8e77e36377f8c2c9194e8bb69035ca7c53d8d","trusted":false,"collapsed":true},"cell_type":"code","source":"data.isnull().sum()[data.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"54dc7256-f97a-4f43-a97f-32c5892d1081","_uuid":"cf8f5f15279e264080ae7b69101924fcdd76bf68"},"cell_type":"markdown","source":"### MSZoning, Utilities, Exteriors, Electrical, Functional, Utilities and SaleType\n\nThere are not many missing values, so we will just go with the mode of the neighbourhood."},{"metadata":{"_cell_guid":"6c0c9f6d-2c92-49d4-b951-4173f752ade5","_uuid":"3bbd8008c779cad8826a6fc512346fa404742419","trusted":false,"collapsed":true},"cell_type":"code","source":"feats = ['MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'Electrical', 'Functional',\n         'SaleType']\nmodel = data.loc['train'].groupby('Neighborhood')[feats].apply(lambda x: x.mode().iloc[0])\n\nfor f in feats:\n    data[f].fillna(data['Neighborhood'].map(model[f]), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dd819073-17ad-41d5-9d10-acc0688ab6ea","_uuid":"d21802d5a700d3c32e6c5dda0f976734d3ee89ee"},"cell_type":"markdown","source":"### Lot frontage\n\nMy best guess is that it depends somewhat strongly on the configuration of the lot (inside/corner/cul/2-frontage/3-frontage)."},{"metadata":{"_cell_guid":"45f3e3ea-1f79-4a60-9623-cceba57f76fe","scrolled":true,"_uuid":"c6a318036cac7cedeba5709cb6cafd34ad51cc33","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.subplots(figsize=(15,5))\nboxdata = data.loc['train'].groupby('LotConfig')['LotFrontage'].median().sort_values(ascending=False)\norder = boxdata.index\nsns.boxplot(x='LotConfig', y='LotFrontage', order=order, data=data.loc['train'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8a83df45-53b0-4316-a156-44f18c7eb198","_uuid":"7c667fed87615feb61d1ea671d09d1d1e481af20","trusted":false,"collapsed":true},"cell_type":"code","source":"data['LotFrontage'] = data['LotFrontage'].fillna(data.loc['train', 'LotFrontage'].median())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c4c0df9e-b9ea-41e2-a79a-88b55a1b5e8f","_uuid":"5fb7c28a679b5d7af1dd76cb5a5177bc5af263dc"},"cell_type":"markdown","source":"### KitchenQual\n\nAgain, very few missing values. We will substitute in this case with the 'OverallQual' value."},{"metadata":{"collapsed":true,"_cell_guid":"faf8fe27-8c4e-4611-a76d-b0d4b3154586","_uuid":"9a100ee929e05e31fb6b2067d81d00e69c429c71","trusted":false},"cell_type":"code","source":"data['KitchenQual'].fillna(data['OverallQual'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1b2cdfa6-7d5d-4f75-9d65-dfe17fd69088","_uuid":"7eaa2fc4cfe07d6cb8c1fabb53bf7a0e63e4ebe4"},"cell_type":"markdown","source":"### Basement, garage, fireplaces and other features\n\nWe can interpret an NA in all these things as \"the house does not have this feature\"."},{"metadata":{"collapsed":true,"_cell_guid":"2aff99c8-d9e2-46db-aa68-3fc7b24f68fc","_uuid":"b0760cc73e30583342af9cdcacfcbc5c2f5a6c6b","trusted":false},"cell_type":"code","source":"bsmt = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n        'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFullBath',\n        'BsmtHalfBath', \n        'TotalBsmtSF']\nfire = ['Fireplaces', 'FireplaceQu']\ngarage = ['GarageQual', 'GarageCond', 'GarageType', 'GarageFinish', 'GarageCars', \n          'GarageArea', 'GarageYrBlt']\nmasn = ['MasVnrType', 'MasVnrArea']\nothers = ['Alley', 'Fence', 'PoolQC', 'MiscFeature']\n\ncats = data.columns[data.dtypes == 'object']\nnums = list(set(data.columns) - set(cats))\n\n# Be sure the category 'None' is also handled here\ndata['MasVnrType'].replace({'None': np.nan}, inplace=True)\n\ndata[cats] = data[cats].fillna('0')\ndata[nums] = data[nums].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ed96cc23-d59b-4691-80b9-a2c67910fa48","_uuid":"b1bbbb2bb9e5a88d4bbee942e267af6638acdf34","trusted":false,"collapsed":true},"cell_type":"code","source":"data.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a0aabafd-44d9-4e9c-bdea-2cb400b66381","_uuid":"3231b1dd92515e5b44f888aaa076918228adf8c5"},"cell_type":"markdown","source":"### Adjusting the type of variable\n\nFirst, there are a few features that are not represented with the right type of variable:\n\n- 'MSSubClass': represented as an integer, when it is just a category label (we will use 'object' for now)\n- 'MoSold': represented as an integer, a month is just a category label out of 12 possibilities (we will use 'object' for now)\n- 'BsmtFullBath', 'BsmtHalfBath': these two represent integers and not floats (or I at least I do not know what a third of half bathroom is)\n- years: a year, in the context of this dataset, is an integer, and not a float\n- 'GarageCars': represented as a float, it is an actual integer (nobody wants to have 0.5 car at home)"},{"metadata":{"collapsed":true,"_cell_guid":"32071d3d-eb66-4dc2-a9c4-3796c3dc9143","scrolled":true,"_uuid":"4a9907f29f9a404eb2c8c6fc7e9d81452b6741ba","trusted":false},"cell_type":"code","source":"data['MSSubClass'] = data['MSSubClass'].astype('object', copy=False)\ndata['MoSold'] = data['MoSold'].astype('object', copy=False)\ndata['BsmtFullBath'] = data['BsmtFullBath'].astype('int64', copy=False)\ndata['BsmtHalfBath'] = data['BsmtHalfBath'].astype('int64', copy=False)\ndata['GarageCars'] = data['GarageCars'].astype('int64', copy=False)\ndata[years] = data[years].astype('int64', copy=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a3359afb-2849-4513-b7b2-9dff1a559d1f","_uuid":"bdbc6700857412181d639aa3b78193054b234963"},"cell_type":"markdown","source":"### Categorical data with few samples per bin\n\nSome categories of the categorical features are so unrrepresented in the dataset that drawing conclusions from them would lead to a noisy result. Instead, we will group those in one single category."},{"metadata":{"collapsed":true,"_cell_guid":"b8ce1510-c904-41bd-9a9d-314b26a26cce","_uuid":"e085f25f854000b4338a3180d2f652eba43a0970","trusted":false},"cell_type":"code","source":"categorical_data = pd.concat((data.loc['train'][cat_feats], price), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5459bb36-bfe7-420c-9699-ae35bc65b018","scrolled":true,"_uuid":"f97b27ff5d2d5b29d8575bf6ce36437fb2914f00","trusted":false,"collapsed":true},"cell_type":"code","source":"low = 0.05 * data.loc['train'].shape[0] # at least 5% of the dataset should have this value\n\nfor feat in cat_feats:        \n    # we will group the categories based on the average sale price\n    order = ((categorical_data.groupby(feat).mean()).sort_values(by='SalePrice', \n                                                      ascending=False).index.values.tolist())\n    for i in range(0, len(order)):\n        N = (categorical_data[categorical_data[feat] == order[i]]\n             .count().max())\n        j = i\n        while (N < low) & (N != 0):\n            j += 1\n\n            if (j > len(order) - 1):\n                # if reached the end of list, go back to last\n                # useful category of the 'order' list\n                j = i - 1\n                break\n            else: \n                N += (categorical_data[categorical_data[feat] == order[j]]\n                      .count().max())\n        if j < i:\n            lim = len(order)\n        else:\n            lim = j\n\n        for k in range(i, lim):\n            categorical_data.replace({feat: {order[k]: order[j]}},\n                                 inplace=True)\n            data.replace({feat: {order[k]: order[j]}},\n                                     inplace=True)            \n    uniD = data[feat].unique()\n    order = categorical_data[feat].unique()\n\n    for i in uniD:\n        if i not in order:\n            ind = np.argsort(order - i)[0]\n            data.replace({feat: {i: order[ind]}}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"45a43cf2-eb10-4392-ab3c-ec5a70917c88","_uuid":"7ba2b75832a1245b3f93a453f728f94696dc883e","trusted":false,"collapsed":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"08e8c225-6800-4b84-b363-4b059e86dec9","_uuid":"c262158151d9c4b50eb30e6744c182f807e1a327"},"cell_type":"markdown","source":"## D. Dummy encoding\n\n### Categorical data as categories\n\nFirst, we need to turn those features with two categories into 0-1 encoding (the get_dummies method would convert them into two separate features otherwise, feat_0 and feat_1)."},{"metadata":{"_cell_guid":"2e7d9cba-9a4d-4458-a532-98e1feab7590","scrolled":true,"_uuid":"a1354bc9fb19b8954d622b3752b2be5c6b32be8c","trusted":false,"collapsed":true},"cell_type":"code","source":"# Remove columns with just one category\nfor feat in categorical_data.columns[:-1]:    \n    uni = categorical_data.groupby(feat).mean().sort_values(by='SalePrice').index\n    if (len(uni) < 2):\n            data.drop(feat, axis=1, inplace=True)\n    elif len(uni) < 3:\n        print(\"{}: {}\".format(feat, uni))\n        data[feat].replace({uni[0]: 0, uni[1]: 1}, inplace=True)\n        data[feat] = data[feat].astype('int8')\n    else:\n        data[feat] = data[feat].astype('category')\n        ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e7ec4d7e-d6fd-4736-a512-13048a3f2a29","_uuid":"db70c742b5570f5bc4bd637b1e485b61395b7b37","trusted":false},"cell_type":"code","source":"finaldata = pd.get_dummies(data)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f6539f74-c4ea-4e8e-8ccb-162e1c06528f","_uuid":"34ff48e2e98d11d3bec574c5d256c3aa838ca38f"},"cell_type":"markdown","source":"The variables that express a \"I don't have this\" feature should not treat the '0' as a normal category. Instead, it would be cleaner (and less overfitting) to encode the zero in the other possible options."},{"metadata":{"collapsed":true,"_cell_guid":"88e4ea80-66f4-41b4-a51c-72d735420304","scrolled":true,"_uuid":"48abc075ea31647bfd0aa29530c5eeb263ff9fed","trusted":false},"cell_type":"code","source":"black_list = bsmt + fire + garage + masn + others\nfor feat in finaldata.columns:\n    if ('_0' in feat) and (feat.split(\"_\")[0] in black_list):\n        finaldata.drop(feat, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9a8c8b57-be75-44d2-923a-f295d91b74ba","scrolled":false,"_uuid":"85a61a2dafa7d6991c65f583feb1d2cd249c05a7","trusted":false,"collapsed":true},"cell_type":"code","source":"finaldata.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fccd6723-f248-47bc-afaa-4dc75a7f4667","_uuid":"3a3a1133102a227e99ebc2b61cf30789350284c4"},"cell_type":"markdown","source":"## E. Rescaling\n\nI will just separate the data and normalise it to make the regressors run smoother. As @Gennadi mentioned in the comments, I have to be careful not to leak any data from the test set into my training set. \n\nThat is: I can only use the mean and standard dev of my train set to normalise."},{"metadata":{"collapsed":true,"_cell_guid":"f0c36ce0-0af2-44b6-bb54-634201090cae","_uuid":"a77c2a464ce2db3c4eb7ee0b2ad2a5cab4d81c0b","trusted":false},"cell_type":"code","source":"# Training/testing sets\nX_test = finaldata.loc['test']\nX_train = finaldata.loc['train']\n\ny_train = price","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"28c43be2-9977-465d-95bb-c5ebc21bc955","_uuid":"a63514e56051b66f88d08b03a0609c6a4b72adf7","trusted":false},"cell_type":"code","source":"m = X_train.mean()\nstd = X_train.std()\n\nX_train = (X_train - m) / std\nX_test = (X_test - m) / std","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9d327d64-bac1-4859-9aa7-d5e831e3e669","_uuid":"53b4d8b48d6f90e381197e8546f910007a5a9b87"},"cell_type":"markdown","source":"# 2. Regression\n"},{"metadata":{"_cell_guid":"a5b5510e-700f-4580-9f76-6ea61f8f9539","_uuid":"4e7e6cdd915db0e963fda49f9005b13fa2d5ad59"},"cell_type":"markdown","source":"## A. Linear regression (without regularisation)"},{"metadata":{"_cell_guid":"598c9dec-e447-46d8-8cdd-f2fd8927defe","_uuid":"3359c4b797d482e33771815a342db4c395773407","trusted":false,"collapsed":true},"cell_type":"code","source":"# Create linear regression object\nLR = LinearRegression()\n\n# Train the model using the training sets\nLR.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"519da34c-dc54-4617-b301-ceebd3e9622c","_uuid":"77f2e43450fdd5b2adac680cc77c698f764133b5"},"cell_type":"markdown","source":"### Top influencers"},{"metadata":{"_cell_guid":"7b2ae96c-5f51-4811-b664-4734a3781df9","scrolled":true,"_uuid":"9fd5eb26dbc613955fc98011742cab84171ca27a","trusted":false,"collapsed":true},"cell_type":"code","source":"maxcoef = np.argsort(-np.abs(LR.coef_))\ncoef = LR.coef_[maxcoef]\nfor i in range(0, 5):\n    print(\"{:.<025} {:< 010.4e}\".format(finaldata.columns[maxcoef[i]], coef[i]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ef6d8ba2-4481-40ca-a728-d922cff283e9","_uuid":"150924ddab7bda8128832881d907afb3e86105cd"},"cell_type":"markdown","source":"The coefficients indicate that this model is <u>very</u> overfitted. We can blame this on the correlation of several of the features:\n\n1. Dummy features are by definition correlated (i.e. if LotShape_Reg = 1, we can be certain that LotShape_IR1 = 0 for that house)\n2. There might be other correlated features: 'GrLivArea' is probably very close to the sum of the first and second floor areas."},{"metadata":{"_cell_guid":"f282cb54-e850-4883-b947-642c747852b0","_uuid":"4822d30116880270bda40f68f384c7ed4355b5a1"},"cell_type":"markdown","source":"## B. Linear regression, L1 regularisation"},{"metadata":{"_cell_guid":"d74e4f46-d84c-491e-85d3-b136682f87f5","scrolled":true,"_uuid":"d09aed76ef8544e12a637dbf9c35b4bc54808361","trusted":false,"collapsed":true},"cell_type":"code","source":"# Create linear regression object\nLs = LassoCV()\n\n# Train the model using the training sets\nLs.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d6bdfe08-e6c0-46ca-a2fa-354b256b5fd0","_uuid":"30740815e253e2166e9cba256df22fabb20e6474"},"cell_type":"markdown","source":"### Top influencers"},{"metadata":{"_cell_guid":"29bc7eb3-1718-4f14-9046-f9f9e704ff62","scrolled":false,"_uuid":"739a1f3667ab6e64486b82659d95cf6ff7b6423b","trusted":false,"collapsed":true},"cell_type":"code","source":"maxcoef = np.argsort(-np.abs(Ls.coef_))\ncoef = Ls.coef_[maxcoef]\nfor i in range(0, 5):\n    print(\"{:.<025} {:< 010.4e}\".format(finaldata.columns[maxcoef[i]], coef[i]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1fa41329-cada-4d48-9a6d-9ccf506ea5af","_uuid":"ddeab627e7231c3bff4f02c441e8a88c90b1ee49"},"cell_type":"markdown","source":"This looks a lot more reasonable. When one thinks about the price of a house, lot area and living area are usually the first guess."},{"metadata":{"_cell_guid":"a69936c0-c91c-4b25-94c4-df064276e670","_uuid":"5626de0cfae1b93057f523ad36814e8cb9f70305"},"cell_type":"markdown","source":"## C. Linear regression, L2 regularisation"},{"metadata":{"_cell_guid":"66d4a2e5-2ddb-49e3-b8cd-d4225bfafeff","_uuid":"6be9d127a4372efd59f038a20a14c4987a51b0ff","trusted":false,"collapsed":true},"cell_type":"code","source":"# Create linear regression object\nRr = RidgeCV()\n\n# Train the model using the training sets\nRr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"28509802-9811-4720-8551-643c3ba59fb2","_uuid":"45c41ba04453269223b531859c9a6f97a2f77f8a"},"cell_type":"markdown","source":"### Top influencers"},{"metadata":{"_cell_guid":"db5c80df-8283-4d1b-88a8-4fc1f502bae2","scrolled":true,"_uuid":"bcb5968c6b1249035127f20f6208cea20ad35c93","trusted":false,"collapsed":true},"cell_type":"code","source":"maxcoef = np.argsort(-np.abs(Rr.coef_))\ncoef = Rr.coef_[maxcoef]\nfor i in range(0, 5):\n    print(\"{:.<025} {:< 010.4e}\".format(finaldata.columns[maxcoef[i]], coef[i]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"15b1d908-4f68-41b8-9ca7-43188cc97ad3","_uuid":"ca15a2c96930f2cdaeb5738f037015448b9c52d8"},"cell_type":"markdown","source":"This is also very consistent with the L1 regularisation."},{"metadata":{"_cell_guid":"a69936c0-c91c-4b25-94c4-df064276e670","_uuid":"5626de0cfae1b93057f523ad36814e8cb9f70305"},"cell_type":"markdown","source":"## D. Linear regression, elastic net\n\nOkay, so one last attempt is to linearly combine Lasso and Ridge regularisations together (what they fancily call elastic net). The advantage of the elastic net is that if two features are correlated, it will retain both instead of just one (remember that in L1 regularisation, most of the features are set to parameter 0). "},{"metadata":{"collapsed":true,"_cell_guid":"66d4a2e5-2ddb-49e3-b8cd-d4225bfafeff","_uuid":"6be9d127a4372efd59f038a20a14c4987a51b0ff","trusted":false},"cell_type":"code","source":"# Create linear regression object\nEN = ElasticNetCV(l1_ratio=np.linspace(0.1, 1.0, 5)) # we are essentially smashing most of the Rr model here\n\n# Train the model using the training sets\ntrain_EN = EN.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"28509802-9811-4720-8551-643c3ba59fb2","_uuid":"45c41ba04453269223b531859c9a6f97a2f77f8a"},"cell_type":"markdown","source":"### Top influencers"},{"metadata":{"_cell_guid":"db5c80df-8283-4d1b-88a8-4fc1f502bae2","scrolled":true,"_uuid":"bcb5968c6b1249035127f20f6208cea20ad35c93","trusted":false,"collapsed":true},"cell_type":"code","source":"maxcoef = np.argsort(-np.abs(EN.coef_))\ncoef = EN.coef_[maxcoef]\nfor i in range(0, 5):\n    print(\"{:.<025} {:< 010.4e}\".format(finaldata.columns[maxcoef[i]], coef[i]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"26ac4f56-af4f-499d-8a67-bd8c226ee29f","_uuid":"d772e4401bfae11b61788a3034bfa1da6eadfaf9"},"cell_type":"markdown","source":"## Comparison"},{"metadata":{"collapsed":true,"_cell_guid":"5f58944b-e083-499d-b99b-9f46bc36146d","scrolled":true,"_uuid":"953ef364c97372b67d055952ed9e718ac63e2251","trusted":false},"cell_type":"code","source":"model = [Ls, Rr, EN]\nM = len(model)\nCV = 5\nscore = np.empty((M, CV))\nfor i in range(0, M):\n    score[i, :] = cross_val_score(model[i], X_train, y_train, cv=CV)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ae3dc752-5bc7-4723-87f0-627bf6cc4144","_uuid":"8fecaf1dadaee7d5021545fe82cbfb222a314a2b","trusted":false},"cell_type":"code","source":"print(score.mean(axis=1))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"79e4b1a8-e657-4d49-aed7-4e8a97101d2a","_uuid":"1140b0fe91caa77b9131152d1de2384774aa2a0b","trusted":false},"cell_type":"code","source":"submit = pd.DataFrame({'Id': testID, 'SalePrice': np.exp(EN.predict(X_test))})\nsubmit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8c9fed02-c7ba-40c6-b1a2-b18229e694bd","_uuid":"c70883b267bbf0524f228256286fc5db3caa0bae"},"cell_type":"markdown","source":"A few conclusions from here:\n\n- Linear regression, without regularisation, cannot fit this data \"as-is\". I believe it is because of the strong correlation between some of the variables with each other\n- Lasso and Ridge regression work both fairly well. The elastic net does not improve the performance that much"},{"metadata":{"_cell_guid":"0432c118-5827-46f0-8c92-9cf057e886fb","_uuid":"a5044b465a81069fa81d2dc35dad9990d192074c"},"cell_type":"markdown","source":"### Future improvements\n\n- Feature engineering: in general, it is not a good idea to feed correlated features to a linear regression model. A way forward could be some deeper analysis of the variables\n- These are the simplest supervised learning models out there. My next try will probably be tree-based solutions, which tend to do better in this kind of datasets\n\nAnd, that's it. If you read this and have some ideas for improvement, please let me know!"}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}