{"cells":[{"metadata":{"_uuid":"0c216cda7577b618589d379d78ee6d9c4710dce0"},"cell_type":"markdown","source":"# Regression Guide to Predict House Price\nHi! If you read my previous [kernel](https://www.kaggle.com/samsonqian/titanic-guide-with-sklearn-and-eda) of predicting survivors of the Titanic, you saw a thorough guide of EDA, visualization, and classification. This kernel is going to be about another kind of Supervised Machine Learning, regression, where we predict numeric values (house price) instead of categories (survived/didn't survive). This guide will go more in depth of data preprocessing and modelling, because the data set we will work with is much larger and complex than the Titanic data. Let's get started! \n\n*Please upvote if this kernel helps you! Feel free to fork this notebook to play with the code yourself.* If you may have any questions about the code, or any step of the process, please comment and I will clear up any confusion."},{"metadata":{"_uuid":"36679501fdee92f1a07d283c3012dfe34378ad7b"},"cell_type":"markdown","source":"My next kernel will be about Deep Learning and Neural Networks, so please follow me and stay tuned for that!"},{"metadata":{"_uuid":"c29ac6032c805a2c2fcca3708fdab8fdc301d9a2"},"cell_type":"markdown","source":"## Classification vs. Regression\nThe problem we are dealing with in this kernel is predicting house prices from features of the house (ie. how many rooms it has). Because we are trying to predict a continuous value instead of a binary value (ie. Titanic survivors), this is a regression problem. For a guide of classification, please visit [here](https://www.kaggle.com/samsonqian/titanic-guide-with-sklearn-and-eda)."},{"metadata":{"_uuid":"4e109248a1b9942ece97521ec8d74af130f6e0c0"},"cell_type":"markdown","source":"# Contents\n1. [Importing Packages](#p1)\n2. [Loading and Inspecting Data](#p2)\n3. [Imputing Null Values](#p3)\n4. [Feature Engineering](#p4)\n5. [Creating, Training, Evaluating, Validating, and Testing ML Models](#p5)\n6. [Submission](#p6)"},{"metadata":{"_uuid":"524bbbc49f9e71dd82aea24e3bf31885c9c951ae"},"cell_type":"markdown","source":"<a id=\"p1\"></a>\n# 1.  Importing Packages\nWe use the same modules as we would use for any problem working with data. We have numpy and pandas to work with numbers and data, and we have seaborn and matplotlib to visualize data. We would also like to filter out unnecessary warnings."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nsns.set(style=\"whitegrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bda3e21a91b2b8904a0ebcf41311edc3ee64ab86"},"cell_type":"markdown","source":"<a id=\"p2\"></a>\n# 2. Loading and Inspecting Data\nWith various Pandas functions, we load our training and test data set as well as inspect it to get an idea of the data we're working with. Wow! That is a large data set; just take a look at its shape. We're going to have to understand our data before modelling. "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"training = pd.read_csv(\"../input/train.csv\")\ntesting = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8fe1c42bc857b325c7c03cd514232658a214995"},"cell_type":"code","source":"training.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4df627b50ca3b192f980917cbbcf8dde30a06189"},"cell_type":"code","source":"training.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3e89246f3f4d888af881f72ffaf583b79b8cf4b"},"cell_type":"code","source":"training.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2854368ad774aa0a0edb00547c7e42290f6c2f47"},"cell_type":"markdown","source":"> That is a very large data set! We are going to have to do a lot of work to clean it up"},{"metadata":{"trusted":true,"_uuid":"eda9a9d028ba4ca03cf4fcaaa679fc20dffbe340"},"cell_type":"code","source":"training.keys()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e93f1729cf83402d20f222e3faf0e0677480666d"},"cell_type":"markdown","source":"Since there are so many columns to work with, let's inspect the correlations to get a better idea of which columns correlate the most with the Sale Price of the house. If there are features that don't do a good job predicting the Sale Price, we can just eliminate them and not use them in our model."},{"metadata":{"trusted":true,"_uuid":"ee608f1685316dcf83e9006d46f4b7bdad4dd877"},"cell_type":"code","source":"correlations = training.corr()\ncorrelations = correlations[\"SalePrice\"].sort_values(ascending=False)\nfeatures = correlations.index[1:6]\ncorrelations","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b412cc2ded1316b7c78138d20f19635e96b5f5e"},"cell_type":"markdown","source":"<a id=\"p3\"></a>\n# 3. Imputing Null Values\nWith data this large, it is not surprising that there are a lot of missing values in the cells. In order to effectively train our model we build, we must first deal with the missing values. There are missing values for both numerical and categorical data. We will see how to deal with both.\n\nFor numerical imputing, we would typically fill the missing values with a measure like median, mean, or mode. For categorical imputing, I chose to fill the missing values with the most common term that appeared from the entire column. There are other ways to do the imputing though, and I ecnourage you to test out your own creative ways!"},{"metadata":{"_uuid":"3ea447ed6347b6dee1471a1807f6543b7fafe318"},"cell_type":"markdown","source":"## Places Where NaN Means Something\nIf you look at the data description file provided, you will see that for some categories, NaN actually means something. This means that if a value is NaN, the house might not have that certain attribute, which will affect the price of the house. Therefore, it is better to not drop, but fill in the null cell with a value called \"None\" which serves as its own category."},{"metadata":{"trusted":true,"_uuid":"f0b970ef67e0c992655fa77f13b6798a21ec6f5e"},"cell_type":"code","source":"training_null = pd.isnull(training).sum()\ntesting_null = pd.isnull(testing).sum()\n\nnull = pd.concat([training_null, testing_null], axis=1, keys=[\"Training\", \"Testing\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dae02dd53753b659156350e7d52adbb5c4ed8a89"},"cell_type":"code","source":"null_many = null[null.sum(axis=1) > 200]  #a lot of missing values\nnull_few = null[(null.sum(axis=1) > 0) & (null.sum(axis=1) < 200)]  #not as much missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df80e4cbda0c9bd46aac0b618cac2d661ece4a73"},"cell_type":"code","source":"null_many","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4031fd7caa6a849adb98990662b93af8a05f8514"},"cell_type":"code","source":"#you can find these features on the description data file provided\n\nnull_has_meaning = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1461b02b014f84bb45ec4b830e4ff0a5b5f7d846"},"cell_type":"code","source":"for i in null_has_meaning:\n    training[i].fillna(\"None\", inplace=True)\n    testing[i].fillna(\"None\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5859aa762ed42d25fd778fea8bd705e5c9d0f5b8"},"cell_type":"markdown","source":"## Imputing \"Real\" NaN Values\nThese are the real NaN values that we have to deal with accordingly because they were not recorded."},{"metadata":{"trusted":true,"_uuid":"8a714fe0e45e6e3564757158253ca4bd4cfd3680"},"cell_type":"code","source":"from sklearn.preprocessing import Imputer\n\nimputer = Imputer(strategy=\"median\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef993a27d5e07faf0b78e0926a01d01f39c1e0bd"},"cell_type":"code","source":"training_null = pd.isnull(training).sum()\ntesting_null = pd.isnull(testing).sum()\n\nnull = pd.concat([training_null, testing_null], axis=1, keys=[\"Training\", \"Testing\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92a654e695c4978cc9c658d7eb3bf88512b9189b"},"cell_type":"code","source":"null_many = null[null.sum(axis=1) > 200]  #a lot of missing values\nnull_few = null[(null.sum(axis=1) > 0) & (null.sum(axis=1) < 200)]  #few missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf828cea569d45e9ae5f670717fc3dc2cc48edc9"},"cell_type":"code","source":"null_many","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"394b8beb1e57fa4182810bc3a18579ddf1e0c9a4"},"cell_type":"markdown","source":"LotFrontage has too many Null values and it is a numerical value so it may be better to just drop it."},{"metadata":{"trusted":true,"_uuid":"3c43dafec9ec8decf974f398df14aee0fdcce368"},"cell_type":"code","source":"training.drop(\"LotFrontage\", axis=1, inplace=True)\ntesting.drop(\"LotFrontage\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95a65f8a438a2705e1a91b1f040b3e4e0c4336ad"},"cell_type":"code","source":"null_few","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c3c2fddd196c0001a8a44912fc6d7cf6ec304a6"},"cell_type":"markdown","source":"GarageYrBlt, MasVnrArea, and MasVnrType all have a fairly decent amount of missing values. MasVnrType is categorical so we can replace the missing values with \"None\", as we did before. We can fill the others with median."},{"metadata":{"trusted":true,"_uuid":"d82945e73cd7179d54488adbf9a41be12f41ed94"},"cell_type":"code","source":"training[\"GarageYrBlt\"].fillna(training[\"GarageYrBlt\"].median(), inplace=True)\ntesting[\"GarageYrBlt\"].fillna(testing[\"GarageYrBlt\"].median(), inplace=True)\ntraining[\"MasVnrArea\"].fillna(training[\"MasVnrArea\"].median(), inplace=True)\ntesting[\"MasVnrArea\"].fillna(testing[\"MasVnrArea\"].median(), inplace=True)\ntraining[\"MasVnrType\"].fillna(\"None\", inplace=True)\ntesting[\"MasVnrType\"].fillna(\"None\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da6a948ad2bd19af0e0ba6d29ff6508b39cb6550"},"cell_type":"markdown","source":"Now, the features with a lot of missing values have been taken care of! Let's move on to the features with fewer missing values."},{"metadata":{"trusted":true,"_uuid":"b2d1b4886fbd20121f06e0882b33dc781c92ace8"},"cell_type":"code","source":"types_train = training.dtypes #type of each feature in data: int, float, object\nnum_train = types_train[(types_train == int) | (types_train == float)] #numerical values are either type int or float\ncat_train = types_train[types_train == object] #categorical values are type object\n\n#we do the same for the test set\ntypes_test = testing.dtypes\nnum_test = types_test[(types_test == int) | (types_test == float)]\ncat_test = types_test[types_test == object]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd7875ae29163221e43e36647363ad419324ce05"},"cell_type":"markdown","source":"**Numerical Imputing**\n\nWe'll impute with median since the distributions are probably very skewed."},{"metadata":{"trusted":true,"_uuid":"966d0d05594d76b84200c403b968a08a8b0a7fd4"},"cell_type":"code","source":"#we should convert num_train and num_test to a list to make it easier to work with\nnumerical_values_train = list(num_train.index)\nnumerical_values_test = list(num_test.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2024a32642ce9001b752388a15715a0e3ea11b7c"},"cell_type":"code","source":"print(numerical_values_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7fccf09f340f792a8f1db8ae538bf9864c0eb92"},"cell_type":"markdown","source":">These are all the numerical features in our data."},{"metadata":{"trusted":true,"_uuid":"417fcd98e5271a221a7129c0916c0e9074274f8a"},"cell_type":"code","source":"fill_num = []\n\nfor i in numerical_values_train:\n    if i in list(null_few.index):\n        fill_num.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ebf1648208aba942d0ce45bb5f93e9cc74142d0"},"cell_type":"code","source":"print(fill_num)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77077aa121977ce9c3df66992042c3973fbcccd1"},"cell_type":"markdown","source":">These are the numerical features in the data that have missing values in them. We will impute these features with a for-loop below. "},{"metadata":{"trusted":true,"_uuid":"bf7535fc47843b1431730cada4ad5a2f7427af6e"},"cell_type":"code","source":"for i in fill_num:\n    training[i].fillna(training[i].median(), inplace=True)\n    testing[i].fillna(testing[i].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca36b01c43ea6db57a777440b33de845ac0722f2"},"cell_type":"markdown","source":"**Categorical Imputing**\n\nSince these are categorical values, we can't impute with median or mean. We can, however, use mode. We'll impute with the most common term that appears in the entire list."},{"metadata":{"trusted":true,"_uuid":"6d690ad17ac6500fd6e9486a489d24a11f7cda48"},"cell_type":"code","source":"categorical_values_train = list(cat_train.index)\ncategorical_values_test = list(cat_test.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffce7f4da9f1235270f7288144d084ce318c370f"},"cell_type":"code","source":"print(categorical_values_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"329e8abdab3475262051bbc50b609305d725f69f"},"cell_type":"markdown","source":">These are all the categorical features in our data"},{"metadata":{"trusted":true,"_uuid":"306c5fbfe0d54b91541a209a25b8412554386387"},"cell_type":"code","source":"fill_cat = []\n\nfor i in categorical_values_train:\n    if i in list(null_few.index):\n        fill_cat.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"474d108a431a227f1e2bbea1b3a38c3f46fb19af"},"cell_type":"code","source":"print(fill_cat)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9d86f7f4359127f08df0152d96acec3b18696a7"},"cell_type":"markdown","source":">These are the categorical features in the data that have missing values in them. We'll impute with the most common term below. "},{"metadata":{"trusted":true,"_uuid":"076a68b5e0680ba5cee6986cfbc3bf6c8753b2ea"},"cell_type":"code","source":"def most_common_term(lst):\n    lst = list(lst)\n    return max(set(lst), key=lst.count)\n#most_common_term finds the most common term in a series\n\nmost_common = [\"Electrical\", \"Exterior1st\", \"Exterior2nd\", \"Functional\", \"KitchenQual\", \"MSZoning\", \"SaleType\", \"Utilities\", \"MasVnrType\"]\n\ncounter = 0\nfor i in fill_cat:\n    most_common[counter] = most_common_term(training[i])\n    counter += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8ab27b4a060651fd235b62c7ae7f0c8a93fffab"},"cell_type":"code","source":"most_common_dictionary = {fill_cat[0]: [most_common[0]], fill_cat[1]: [most_common[1]], fill_cat[2]: [most_common[2]], fill_cat[3]: [most_common[3]],\n                          fill_cat[4]: [most_common[4]], fill_cat[5]: [most_common[5]], fill_cat[6]: [most_common[6]], fill_cat[7]: [most_common[7]],\n                          fill_cat[8]: [most_common[8]]}\nmost_common_dictionary","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0da65b1a6b6d22a593c6093fe034ece2e50c825"},"cell_type":"markdown","source":">This shows the most common term for each of the categorical features that we're working with. We'll replace the null values with these."},{"metadata":{"trusted":true,"_uuid":"66b51b15a932549107b8d714fa0be0c50e024785"},"cell_type":"code","source":"counter = 0\nfor i in fill_cat:  \n    training[i].fillna(most_common[counter], inplace=True)\n    testing[i].fillna(most_common[counter], inplace=True)\n    counter += 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77d3f8e81987d8beb329ff92c54975d954cf4d4d"},"cell_type":"markdown","source":"Good! That should take care of the last couple of missing values. Let's check our work by looking at how many null values remain. If we are successful, the code below should print an empty table."},{"metadata":{"trusted":true,"_uuid":"5402f791fccd94ab9d4d68456d22992e14e38c53"},"cell_type":"code","source":"training_null = pd.isnull(training).sum()\ntesting_null = pd.isnull(testing).sum()\n\nnull = pd.concat([training_null, testing_null], axis=1, keys=[\"Training\", \"Testing\"])\nnull[null.sum(axis=1) > 0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d576bc7317a395f8a0e8c56aef2f8ec544fd0ac"},"cell_type":"markdown","source":"Yup! An empty table."},{"metadata":{"_uuid":"11fe20e82e11e2394a916cb2b439b01a63ea0ae2"},"cell_type":"markdown","source":"<a id=\"p4\"></a>\n# 4. Feature Engineering\nOk, now that we have dealt with all the missing values, it looks like it's time for some feature engineering, the second part of our data preprocessing. We need to create feature vectors in order to get the data ready to be fed into our model as training data. This requires us to convert the categorical values into representative numbers."},{"metadata":{"_uuid":"362bbd386c27317b7bbc7f99243c9f610672be6f"},"cell_type":"markdown","source":"First, let's take a look at our target."},{"metadata":{"trusted":true,"_uuid":"42ffe8ae636e0ca2bbf30328e4696fe7623d1246"},"cell_type":"code","source":"sns.distplot(training[\"SalePrice\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d5526863d281be978cc85d22fc0c2f5818c09cb"},"cell_type":"code","source":"sns.distplot(np.log(training[\"SalePrice\"]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3480496175828473cc7b69c1b75496657815520b"},"cell_type":"markdown","source":"It appears that the target, SalePrice, is very skewed and a transformation like a logarithm would make it more normally distributed. Machine Learning models tend to work much better with normally distributed targets, rather than greatly skewed targets. By transforming the prices, we can boost model performance."},{"metadata":{"trusted":true,"_uuid":"f4fb36cb7a50c4e21601ffb36c54b7c0b41a1e4a"},"cell_type":"code","source":"training[\"TransformedPrice\"] = np.log(training[\"SalePrice\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bae1ad1e76664f449fd75b98450c0894a474610a"},"cell_type":"markdown","source":"Now, let's take a look at all the categorical features in the data that need to be transformed."},{"metadata":{"trusted":true,"_uuid":"1da1c1c390e7cdaa5b83cf0372932767f24028d6"},"cell_type":"code","source":"categorical_values_train = list(cat_train.index)\ncategorical_values_test = list(cat_test.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64a844873e16f656ae206e7e8a8a9b57cb6311de"},"cell_type":"code","source":"print(categorical_values_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f39aef941fb17d25295c66772d94eb42eff27652"},"cell_type":"code","source":"for i in categorical_values_train:\n    feature_set = set(training[i])\n    for j in feature_set:\n        feature_list = list(feature_set)\n        training.loc[training[i] == j, i] = feature_list.index(j)\n\nfor i in categorical_values_test:\n    feature_set2 = set(testing[i])\n    for j in feature_set2:\n        feature_list2 = list(feature_set2)\n        testing.loc[testing[i] == j, i] = feature_list2.index(j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60c5603b72aac552ffc1aafd4294a90b00b6b4e0"},"cell_type":"code","source":"training.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cd70939b4d1b1c0c0242faf54aea2cd59f14924"},"cell_type":"code","source":"testing.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0991cbadc606fce81df462ff588f56eb61d45d6a"},"cell_type":"markdown","source":"Great! It seems like we have changed all the categorical strings into a representative number. We are ready to build our models!"},{"metadata":{"_uuid":"966fc122653f6d3a835417b23e43226e339331c7"},"cell_type":"markdown","source":"<a id=\"p5\"></a>\n# 5. Creating, Training, Evaluating, Validating, and Testing ML Models\nNow that we've preprocessed and explored our data, we have a much better understanding of the type of data that we're dealing with. Now, we can began to build and test different models for regression to predict the Sale Price of each house. We will import these models, train them, and evaluate them. In classification, we used accuracy as a evaluation metric; in regression, we will use the R^2 score as well as the RMSE to evaluate our model performance. We will also use cross validation to optimize our model hyperparameters."},{"metadata":{"trusted":true,"_uuid":"4b7671f5b2488c146aa930d904662fb198bca54d"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.cross_validation import cross_val_score, KFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0063209decd2f48bc2abd58d794a9b0b1375367e"},"cell_type":"markdown","source":"**Defining Training/Test Sets**\n\nWe drop the Id and SalePrice columns for the training set since those are not involved in predicting the Sale Price of a house. The SalePrice column will become our training target. Remember how we transformed SalePrice to make the distribution more normal? Well we can apply that here and make TransformedPrice the target instead of SalePrice. This will improve model performance and yield a much smaller RMSE because of the scale."},{"metadata":{"trusted":true,"_uuid":"5b63a20e96420bbeba7ead9b59911dd70168c839"},"cell_type":"code","source":"X_train = training.drop([\"Id\", \"SalePrice\", \"TransformedPrice\"], axis=1).values\ny_train = training[\"TransformedPrice\"].values\nX_test = testing.drop(\"Id\", axis=1).values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98dfc5af84f59837398820c895637ebb9c5f2ad4"},"cell_type":"markdown","source":"**Splitting into Validation**\n\nIt is always good to split our training data again into validation sets. This will help us evaluate our model performance as well as avoid overfitting our model."},{"metadata":{"trusted":true,"_uuid":"1c68c6cc7a3a9ebcba91afc02d7a0d004be6c64c"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split #to create validation data set\n\nX_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) #X_valid and y_valid are the validation sets","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9cd2334e29af024a2df4e4fb88ffc39e21dbd4e"},"cell_type":"markdown","source":"**Linear Regression Model**"},{"metadata":{"trusted":true,"_uuid":"0498983b82e8f167c69bdb6d67956aec84acfee6"},"cell_type":"code","source":"linreg = LinearRegression()\nparameters_lin = {\"fit_intercept\" : [True, False], \"normalize\" : [True, False], \"copy_X\" : [True, False]}\ngrid_linreg = GridSearchCV(linreg, parameters_lin, verbose=1 , scoring = \"r2\")\ngrid_linreg.fit(X_training, y_training)\n\nprint(\"Best LinReg Model: \" + str(grid_linreg.best_estimator_))\nprint(\"Best Score: \" + str(grid_linreg.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16314f4362d8493a185f16bcee9b3bad4c56f0bf"},"cell_type":"code","source":"linreg = grid_linreg.best_estimator_\nlinreg.fit(X_training, y_training)\nlin_pred = linreg.predict(X_valid)\nr2_lin = r2_score(y_valid, lin_pred)\nrmse_lin = np.sqrt(mean_squared_error(y_valid, lin_pred))\nprint(\"R^2 Score: \" + str(r2_lin))\nprint(\"RMSE Score: \" + str(rmse_lin))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77c1775a60a8ae33b1ce7ee5ffdf5556538da273"},"cell_type":"code","source":"scores_lin = cross_val_score(linreg, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_lin)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00c5348bc2ddfcca0782b3e2c3333c6f95b23bd9"},"cell_type":"markdown","source":"**Lasso Model**"},{"metadata":{"trusted":true,"_uuid":"8c34521180ce0811ccc4e54c39802326e1b77bd2"},"cell_type":"code","source":"lasso = Lasso()\nparameters_lasso = {\"fit_intercept\" : [True, False], \"normalize\" : [True, False], \"precompute\" : [True, False], \"copy_X\" : [True, False]}\ngrid_lasso = GridSearchCV(lasso, parameters_lasso, verbose=1, scoring=\"r2\")\ngrid_lasso.fit(X_training, y_training)\n\nprint(\"Best Lasso Model: \" + str(grid_lasso.best_estimator_))\nprint(\"Best Score: \" + str(grid_lasso.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96ba0bf24bed961a4b7a6e9b6a24c286815765e3"},"cell_type":"code","source":"lasso = grid_lasso.best_estimator_\nlasso.fit(X_training, y_training)\nlasso_pred = lasso.predict(X_valid)\nr2_lasso = r2_score(y_valid, lasso_pred)\nrmse_lasso = np.sqrt(mean_squared_error(y_valid, lasso_pred))\nprint(\"R^2 Score: \" + str(r2_lasso))\nprint(\"RMSE Score: \" + str(rmse_lasso))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d459fb28add653674eb0e10429ebef023c0f780b"},"cell_type":"code","source":"scores_lasso = cross_val_score(lasso, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_lasso)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd99f2bd46d8c854eb9fb0164014c03419968d9a"},"cell_type":"markdown","source":"**Ridge Model**"},{"metadata":{"trusted":true,"_uuid":"a012ca68bbafba865b63b71998e19a354e4df887"},"cell_type":"code","source":"ridge = Ridge()\nparameters_ridge = {\"fit_intercept\" : [True, False], \"normalize\" : [True, False], \"copy_X\" : [True, False], \"solver\" : [\"auto\"]}\ngrid_ridge = GridSearchCV(ridge, parameters_ridge, verbose=1, scoring=\"r2\")\ngrid_ridge.fit(X_training, y_training)\n\nprint(\"Best Ridge Model: \" + str(grid_ridge.best_estimator_))\nprint(\"Best Score: \" + str(grid_ridge.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"552ae4ca5f6f160f60b4bd6e3ab9ae5f0ad34b97"},"cell_type":"code","source":"ridge = grid_ridge.best_estimator_\nridge.fit(X_training, y_training)\nridge_pred = ridge.predict(X_valid)\nr2_ridge = r2_score(y_valid, ridge_pred)\nrmse_ridge = np.sqrt(mean_squared_error(y_valid, ridge_pred))\nprint(\"R^2 Score: \" + str(r2_ridge))\nprint(\"RMSE Score: \" + str(rmse_ridge))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0acdf74ee1138f54079d99a9daa6b4cbfd10666"},"cell_type":"code","source":"scores_ridge = cross_val_score(ridge, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_ridge)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"801cc560adc269856fb7a3a512a15bf58a2c3dc0"},"cell_type":"markdown","source":"**Decision Tree Regressor Model**"},{"metadata":{"trusted":true,"_uuid":"7b6a66e2bb8a29f9a48865e5e89090699aabf53c"},"cell_type":"code","source":"dtr = DecisionTreeRegressor()\nparameters_dtr = {\"criterion\" : [\"mse\", \"friedman_mse\", \"mae\"], \"splitter\" : [\"best\", \"random\"], \"min_samples_split\" : [2, 3, 5, 10], \n                  \"max_features\" : [\"auto\", \"log2\"]}\ngrid_dtr = GridSearchCV(dtr, parameters_dtr, verbose=1, scoring=\"r2\")\ngrid_dtr.fit(X_training, y_training)\n\nprint(\"Best DecisionTreeRegressor Model: \" + str(grid_dtr.best_estimator_))\nprint(\"Best Score: \" + str(grid_dtr.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cb9aed55756f221f2653fe613d59e98b0b0f95c"},"cell_type":"code","source":"dtr = grid_dtr.best_estimator_\ndtr.fit(X_training, y_training)\ndtr_pred = dtr.predict(X_valid)\nr2_dtr = r2_score(y_valid, dtr_pred)\nrmse_dtr = np.sqrt(mean_squared_error(y_valid, dtr_pred))\nprint(\"R^2 Score: \" + str(r2_dtr))\nprint(\"RMSE Score: \" + str(rmse_dtr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2a7438dd8e6623f023e177a31bde268e3c89266"},"cell_type":"code","source":"scores_dtr = cross_val_score(dtr, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_dtr)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c88d1441a14c1ada3593bfaa644434cba5a48cd5"},"cell_type":"markdown","source":"**Random Forest Regressor**"},{"metadata":{"trusted":true,"_uuid":"75f5818049d95a60bdeff2a5ad8a7fa78a5c8d7d"},"cell_type":"code","source":"rf = RandomForestRegressor()\nparemeters_rf = {\"n_estimators\" : [5, 10, 15, 20], \"criterion\" : [\"mse\" , \"mae\"], \"min_samples_split\" : [2, 3, 5, 10], \n                 \"max_features\" : [\"auto\", \"log2\"]}\ngrid_rf = GridSearchCV(rf, paremeters_rf, verbose=1, scoring=\"r2\")\ngrid_rf.fit(X_training, y_training)\n\nprint(\"Best RandomForestRegressor Model: \" + str(grid_rf.best_estimator_))\nprint(\"Best Score: \" + str(grid_rf.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b1c98e7070ba08a40f018a2c271e658d5b55edf"},"cell_type":"code","source":"rf = grid_rf.best_estimator_\nrf.fit(X_training, y_training)\nrf_pred = rf.predict(X_valid)\nr2_rf = r2_score(y_valid, rf_pred)\nrmse_rf = np.sqrt(mean_squared_error(y_valid, rf_pred))\nprint(\"R^2 Score: \" + str(r2_rf))\nprint(\"RMSE Score: \" + str(rmse_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b789425c77987415f039a6bf2e82fca16bb816f1"},"cell_type":"code","source":"scores_rf = cross_val_score(rf, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_rf)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de010b05f05206c469dd1b01e986a9f7bc8fbf1b"},"cell_type":"markdown","source":"## Evaluation Our Models\nNow that we've built and trained a couple of different regression models, let's compare all of them and see which of them is the best one we should use to predict on the test set. Let's create a visualize a table to compare their different evaluation metrics."},{"metadata":{"trusted":true,"_uuid":"9072f1895761acff55f5a915604b3a65b6170fcb"},"cell_type":"code","source":"model_performances = pd.DataFrame({\n    \"Model\" : [\"Linear Regression\", \"Ridge\", \"Lasso\", \"Decision Tree Regressor\", \"Random Forest Regressor\"],\n    \"Best Score\" : [grid_linreg.best_score_,  grid_ridge.best_score_, grid_lasso.best_score_, grid_dtr.best_score_, grid_rf.best_score_],\n    \"R Squared\" : [str(r2_lin)[0:5], str(r2_ridge)[0:5], str(r2_lasso)[0:5], str(r2_dtr)[0:5], str(r2_rf)[0:5]],\n    \"RMSE\" : [str(rmse_lin)[0:8], str(rmse_ridge)[0:8], str(rmse_lasso)[0:8], str(rmse_dtr)[0:8], str(rmse_rf)[0:8]]\n})\nmodel_performances.round(4)\n\nprint(\"Sorted by Best Score:\")\nmodel_performances.sort_values(by=\"Best Score\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"145d5749d8216e4b75ff1d38c6495f202a5dec24"},"cell_type":"code","source":"print(\"Sorted by R Squared:\")\nmodel_performances.sort_values(by=\"R Squared\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"742204fb99329833724309dd903dcbca830589f7"},"cell_type":"code","source":"print(\"Sorted by RMSE:\")\nmodel_performances.sort_values(by=\"RMSE\", ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37d87516180f457bd769bbba6ff6f9f4d5ae8ad2"},"cell_type":"markdown","source":"The RMSEs are small because of the log transformation we performed. So even a 0.1 RMSE may be significant in this case. "},{"metadata":{"_uuid":"b42f5ff06b2af79510a08e107fcdd3abe010ece0"},"cell_type":"markdown","source":"I decided to choose Random Forest Regressor to use on the test set because I believe it will perform the best based on the statistics printed above. It was a high R^2 value and a lower RMSE. Feel free to try another model and let me know if you get even better results!"},{"metadata":{"trusted":true,"_uuid":"db9c1edfe3ec72feb36109762b00c5ede5ba3f2d"},"cell_type":"code","source":"rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0862f477eb559fc59cc1b24fb14d5beaef1a07a6"},"cell_type":"markdown","source":"<a id=\"p6\"></a>\n# 6. Submission\nLet's use our optimized model to predict on the Test Set! We will create a dataframe with the predictions and the IDs to submit."},{"metadata":{"_uuid":"9ea5b66dd15f1386148d5ebebec8dbd201a3088b"},"cell_type":"markdown","source":"Remember how we transformed the Sale Price by taking a log of all the prices? Well, now we need to change that back to the original scale. We can do this with numpy's exp function, which will reverse the log. It is the same as raising *e* to the power of the argument (prediction). (e^pred)"},{"metadata":{"trusted":true,"_uuid":"26ee976ac9f83d52b8a8a7ab48c0aff3de5cd37e"},"cell_type":"code","source":"submission_predictions = np.exp(rf.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4905961d2f0693c3131881bf84f9f99ad391d85"},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"Id\": testing[\"Id\"],\n        \"SalePrice\": submission_predictions\n    })\n\nsubmission.to_csv(\"prices.csv\", index=False)\nprint(submission.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5bb44a3c955e47ecad95f5e7eb5af18c84357da6"},"cell_type":"markdown","source":"If you made it all the way here, thank you and congratulations on learning Regression! Now you should know both types of Supervised Machine Learning. To view my kernel on Classification, click [here](https://www.kaggle.com/samsonqian/titanic-guide-with-sklearn-and-eda) Please upvote and share if this kernel helped you! Also, please feel free to fork this kernel and play around with the code and models. There is always room for improvement in preprocessing and building models. But most importantly, remember that the best way to learn is to perform these projects hands on. Look forward to my future kernels!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}